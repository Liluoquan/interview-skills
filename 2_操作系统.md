# 操作系统

## 基本概念

计算机必要重要的硬件资源无非就是CPU、内存、硬盘、I/O设备。

### 操作系统的功能

操作系统位于硬件资源之上，管理硬件资源，应用程序之下，为应用程序提供服务，同时管理应用程序。

1. **资源分配，资源回收**

   - **资源分配：**体现在CPU上，比如进程调度，多个进程同时请求CPU下，应该给哪一个进程呢？再比如内存分配，内存不够了怎么办？A进程非法访问了B进程的内存地址怎么办？内存内、外碎片问题等。
   - **资源回收：**考虑内存回收后的合并等等。

2. **为应用程序提供服务**

   操作系统将硬件资源的操作封装起来，提供相对统一的接口（系统调用）供开发者调用，避免用户直接访问硬件，造成硬件资源损坏。

3. **管理应用程序**

   即控制进程的生命周期：进程开始时的环境配置和资源分配、进程结束后的资源回收、进程调度等。

4. **内核的功能**

   - **进程调度：**管理进程、线程，决定哪个进程、线程使用CPU。
   - **内存管理能力：**决定内存的分配和回收。
   - **硬件通信能力：**管理硬件，为进程和硬件之间提供通信。
   - **系统调用能力：**应用程序进行更高限权运行的服务，需要系统调用，用户程序和操作系统之间的接口。



### 用户态和内核态





#### 为什么要区分用户态和内核态？

1. **程序之间**：限制不同用户程序之间的访问能力, 防止获取其他用户程序的内存数据。
2. **访问硬件**：避免用户程序直接操作硬件资源，造成硬件损坏。



#### 用户态和内核态的区别？

1. 内核态主要运行操作系统程序，用于操作硬件，并为用户态程序提供操作硬件的接口；而用户态主要运行用户的应用程序。
2. 运行在用户态下的程序不能直接访问操作系统内核数据结构和程序。当我们在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成某些它没有权力和能力完成的工作时需要通过系统调用切换到内核态（比如操作硬件）。
3. 用户态和内核态的特权级不同，内核态的特权级是 R0，用户态是 R3。不同的特权级下可以执行的指令不同。



#### 用户态如何切换到内核态？

1. **系统调用（软件中断）**：系统调用本身就是一种软件中断，中断号是 int0x80，跟硬中断不同。
2. **异常**：当CPU正在执行运行在用户态的程序时，突然发生某些不可预知的异常事件，这个时候就会触发从当前用户态执行的进程转向内核态执行相关的异常事件。例如：缺页异常。
3. **外设中断（硬件中断）**：当外设完成用户的请求时，会向 CPU 发送中断信号，此时 CPU 就会暂停执行下一条即将要执行的指令，转而去执行中断信号对应的处理程序，如果先前执行的指令是在用户态下，则自然就发生从用户态到内核态的转换。属于硬件中断。



### 上下文切换

#### 什么是CPU上下文

CPU 寄存器和程序计数器，因为它们都是 CPU 在执行任务前，必须依赖的环境。

- CPU 寄存器是 CPU 内置的容量小、但速度极快的内存。
- 程序计数器则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。



> CPU 上下文切换的类型

进程上下文切换，线程上下文切换、中断上下文切换



#### 进程上下文切换

> 进程上下文切换的场景

进程的上下文切换实际上就是线程的上下文切换，因为线程是CPU调度的基本单位，而进程只是系统分配资源的基本单位。



#### 线程上下文切换

> 切换场景



1. **时间片耗尽**：为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当某个进程的时间片耗尽了，就会被系统挂起，切换到其它正在等待 CPU 的进程运行。
2. **等待资源**：进程在系统资源不足（比如内存不足）时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行。
3. **主动挂起**：当进程通过睡眠函数 sleep 这样的方法将自己主动挂起时，自然也会重新调度。
4. **被抢占**：当有优先级更高的进程运行时，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行
5. **中断**：发生硬件中断时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序。



> 切换开销

1. **前后两个线程属于不同进程**。此时，因为资源不共享，所以切换过程就跟进程上下文切换是一样。
2. **前后两个线程属于同一个进程**。此时，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据



#### 中断上下文切换

**中断程序是在内核态中进行处理的。**

**对同一个 CPU 来说，中断处理比进程拥有更高的优先级**，所以中断上下文切换并不会与进程上下文切换同时发生。同样道理，由于中断会打断正常进程的调度和执行，所以大部分中断处理程序都短小精悍，以便尽可能快的执行结束。

在 Linux 中，当前进程上下文均保存在进程的任务数据结构中。在发生中断时,内核就在被中断进程的上下文中，在内核态下执行中断服务例程。但同时会保留所有需要用到的资源，以便中断服务结束时能恢复被中断进程的执行。



### 中断

#### 中断的定义

> 中断的种类

1. **通过中断控制器给 CPU 的 INTR 引脚发送信号**，并且允许 CPU 从中断控制器的一个端口上读取中断号，比如按下键盘的一个按键，最终会给到 CPU 一个 0x21 中断号。
2. **CPU 执行某条指令发现了异常**，会自己触发并给自己一个中断号，比如执行到了无效指令，CPU 会给自己一个 0x06 的中断号。
3. **执行 INT n 指令**，会直接给 CPU 一个中断号 n，比如触发了 Linux 的系统调用，实际上就是执行了 INT 0x80 指令，那么 CPU 收到的就是一个 0x80 中断号。



> 中断的定义

1. 中断指的是CPU在执行程序的过程中，由于内外部事件或者程序预先安排的事件，导致CPU暂时停止正在执行的程序，转而执行这些突发事件所对应的服务程序的过程。
2. 可以分为软中断和硬中断两类。



#### 硬中断

由与系统相连的外设（比如网卡、硬盘）自动产生的。主要是用来通知操作系统外设状态发生的变化。比如当网卡收到数据包的时候，就会发出一个中断。硬中断由中断控制器进行管理，硬中断的中断号也是由中断控制器给出的。

Linux下硬中断是可以嵌套的，但是没有优先级的概念，也就是说任何一个新的中断都可以打断正在执行的中断，但同种中断除外。

> 中断控制器

中断控制器相当于一个代理，外部设备产生的中断事件不会直接通过中断总线进入CPU，而是先发送给中断控制器，中断控制器再转交给CPU，避免每个设备对应CPU的一个引脚，主要作用有三个：

- 管理和控制可屏蔽中断
- 对可屏蔽中断进行优先权判定
- 发送中断向量号给CPU



#### 软中断

1. 为了满足实时系统的要求，中断处理应该是越快越好。linux为了实现这个特点，当中断发生的时候，硬中断处理那些短时间就可以完成的工作（比如设置中断寄存器的标志位），而将那些处理事件比较长的工作，放到中断之后来完成，也就是软中断（softirq）来完成。
2. 软中断的具体由一个软中断守护进程执行。
3. 软中断不能嵌套，但相同类型的软中断可以在不同CPU上并行执行。



#### 软中断和硬中断的区别

1. 硬中断是由外设硬件发出的，需要有中断控制器参与。其过程是外设侦测到变化，告知中断控制器，中断控制器通过 CPU 中断引脚通知 CPU，然后硬件进行程序计数器及堆栈寄存器之现场保存工作（引发上下文切换），并根据中断向量调用硬中断处理程序进行中断处理。
2. 软中断则通常是由硬中断处理程序或者进程调度程序等软件程序发出的中断信号，无需中断控制器参与，直接以一个 CPU 指令的形式指示 CPU 进行程序计数器及堆栈寄存器之现场保存工作(亦会引发上下文切换)，并调用相应的软中断处理程序进行中断处理(即我们通常所言之系统调用)。
3. **速度**：硬中断直接以硬件的方式引发，处理速度快。软中断以软件指令之方式适合于对响应速度要求不是特别严格的场景。
4. **屏蔽**：硬中断通过设置 CPU 的屏蔽位可进行屏蔽，软中断则由于是指令之方式给出，不能屏蔽。
5. 硬中断发生后，通常会在硬中断处理程序中调用一个软中断来进行后续工作的处理。
6. 硬中断和软中断均会引起上下文切换(进程/线程之切换)，进程切换的过程是差不多的。





## 内存管理

### 虚拟地址、物理地址、逻辑地址



<img src="https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png" alt="img" style="zoom:80%;" />



> 要点

1. 在Linux下，逻辑地址与虚拟地址/线性地址相同。



#### 逻辑地址

1. 逻辑地址是实际程序所使用的地址，表示实际程序中与段相关的偏移量，具体形式为 `[段标识符：段内偏移量]`
2. 段中的偏移地址，加上相应段的基地址就生成了一个线性地址。



#### 虚拟地址/线性地址

虚拟地址表示的是程序所使用的在虚拟内存中的地址，虚拟地址一般要比物理地址大得多。



#### 物理地址

物理地址是在实际硬件（内存）中的地址。



#### 三者的关系

<img src="https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png" alt="img" style="zoom: 50%;" />

1. 程序中所有的地址实际上都是逻辑地址，需要先通过段表查询出段对应的页表地址，由此将逻辑地址转换为虚拟地址。（段中的偏移地址，加上相应段的基地址就生成了一个线性地址。）
2. 虚拟地址再通过查询页表，找到对应的页表项，得出对应的物理页号，加上对应的页内偏移得到最终的物理地址。



> 当然这个过程是可以通过TLB和MMU来简化的

<img src="https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png" alt="img" style="zoom: 50%;" />

上面整个过程的地址转换都是通过 `MMU` 进行的，CPU只需要告诉MMU需要转换的虚拟地址是什么，然后由MMU进行查询

1. 先在TLB中查询（如果存在，则直接返回）
2. 否则，通过页表去查（一级页表，二级页表），此时如果页表中不存在对应的虚拟地址，则表明地址编码错误，直接触发段错误结束进程；存在对应虚拟地址但不存在对应的物理页，则触发缺页中断，让CPU进行物理页分配。
3. 根据虚拟地址找到对应的物理页号，加上偏移量就是最后的物理地址了。







### 虚拟内存

> 虚拟地址的作用

- 避免用户直接访问物理内存地址，防止一些破坏性操作，保护操作系统
- 每个进程都被分配了4GB的虚拟内存，用户程序可使用比实际物理内存更大的地址空间

`4GB` 的进程虚拟地址空间被分成两部分：「用户空间」和「内核空间」



**操作系统会提供一种机制，将不同进程的虚拟地址和不同内存的物理地址映射起来。**

<img src="https://img-blog.csdnimg.cn/72ab76ba697e470b8ceb14d5fc5688d9.png" alt="img" style="zoom: 50%;" />

操作系统引入了虚拟内存，进程持有的虚拟地址会通过 CPU 芯片中的内存管理单元（MMU）的映射关系，来转换变成物理地址，然后再通过物理地址访问内存。

**操作系统是如何管理虚拟地址与物理地址之间的关系？**

主要有两种方式，分别是**内存分段和内存分页**，分段是比较早提出的



#### 内存分段

程序是由若干个逻辑分段组成的，如可由代码分段、数据分段、栈段、堆段组成。**不同的段是有不同的属性的，所以就用分段（*Segmentation*）的形式把这些段分离出来。**

<img src="https://img-blog.csdnimg.cn/c5e2ab63e6ee4c8db575f3c7c9c85962.png" alt="img" style="zoom:80%;" />

> 分段机制下，虚拟地址和物理地址是如何映射的？

分段机制下的虚拟地址由两部分组成，**段选择子**和**段内偏移量**。

<img src="https://img-blog.csdnimg.cn/a9ed979e2ed8414f9828767592aadc21.png" alt="img" style="zoom:80%;" />

- **段选择子**就保存在段寄存器里面。段选择子里面最重要的是**段号**，用作段表的索引。**段表**里面保存的是这个**段的基地址、段的界限和特权等级**等。
- 虚拟地址中的**段内偏移量**应该位于 0 和段界限之间，如果段内偏移量是合法的，就将段基地址加上段内偏移量得到物理内存地址。

虚拟地址是通过**段表**与物理地址进行映射的，分段机制会把程序的虚拟地址分成 4 个段，每个段在段表中有一个项，在这一项找到段的基地址，再加上偏移量，于是就能找到物理内存中的地址

解决了程序本身不需要关心具体的物理内存地址的问题，但它也有一些不足之处：

- 第一个就是**内存碎片**的问题。
- 第二个就是**内存交换的效率低**的问题。

> 分段为什么会产生内存碎片的问题？

![img](https://img-blog.csdnimg.cn/6142bc3c917e4a6298bdb62936e0d332.png)

这里的内存碎片的问题共有两处地方：

- **外部内存碎片**，也就是产生了多个不连续的小物理内存，导致新的程序无法被装载；
- **内部内存碎片**，程序所有各个段（代码段、数据段、堆段、栈段）都被装载到了物理内存，但是这个程序有部分的内存可能并不是很常使用，这也会导致内存的浪费；

**针对外部内存碎片**

使用**内存交换**，将音乐程序占用的那 256MB 内存写到硬盘上，然后再从硬盘上读回来到内存里。（这个用于内存交换的空间，在 Linux 系统里，也就是Swap 空间，这块空间是从硬盘划分出来的，用于内存与硬盘的空间交换。）

对于多进程的系统来说，用分段的方式，内存碎片是很容易产生的，产生了内存碎片，那不得不重新 `Swap` 内存区域，这个过程会产生性能瓶颈。

因为硬盘的访问速度要比内存慢太多了，每一次内存交换，我们都需要把一大段连续的内存数据写到硬盘上。

所以，**如果内存交换的时候，交换的是一个占内存空间很大的程序，这样整个机器都会显得卡顿。**



#### 内存分页

分段的好处就是能产生连续的内存空间，但是会出现内存碎片和内存交换的空间太大的问题。

当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，也就是**内存分页**（*Paging*），**由原来直接映射一段的段表，改为映射一页的页表（一页4KB）**

**分页是把整个虚拟和物理内存空间切成一段段固定尺寸的大小**。这样一个连续并且尺寸固定的内存空间，我们叫**页**（*Page*）。在 Linux 下，每一页的大小为 `4KB`。

<img src="https://img-blog.csdnimg.cn/08a8e315fedc4a858060db5cb4a654af.png" alt="img" style="zoom: 50%;" />

页表是存储在内存里的，**内存管理单元** （*MMU*）就做将虚拟内存地址转换成物理地址的工作。

而当进程访问的虚拟地址在页表中查不到时，系统会产生一个**缺页异常**，进入系统内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。



> 分页是怎么解决分段的内存碎片、内存交换效率低的问题？

<img src="https://img-blog.csdnimg.cn/388a29f45fe947e5a49240e4eff13538.png" alt="img" style="zoom: 50%;" />

由于内存空间都是预先划分好的，也就不会像分段会产生间隙非常小的内存，这正是分段会产生内存碎片的原因。而**采用了分页，那么释放的内存都是以页为单位释放的，也就不会产生无法给进程使用的小内存。**

如果内存空间不够，操作系统会把其他正在运行的进程中的「最近没被使用」的内存页面给释放掉，也就是暂时写在硬盘上，称为**换出**（*Swap Out*）。一旦需要的时候，再加载进来，称为**换入**（*Swap In*）。所以，一次性写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，**内存交换的效率就相对比较高。**



> 分页机制下，虚拟地址和物理地址是如何映射的？

<img src="https://img-blog.csdnimg.cn/7884f4d8db4949f7a5bb4bbd0f452609.png" alt="img" style="zoom:50%;" />

- 把虚拟内存地址，切分成页号和偏移量；
- 根据页号，从页表里面，查询对应的物理页号；
- 直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。

> 单级页表有什么缺陷？

在 32 位的环境下，虚拟地址空间共有 4GB，假设一个页的大小是 4KB（2^12），那么就需要大约 100 万 （2^20） 个页，每个「页表项」需要 4 个字节大小来存储，那么整个 4GB 空间的映射就需要有 `4MB` 的内存来存储页表。

而**每个进程都拥有自己的虚拟地址空间，所以每个进程拥有一个页表**。`100` 个进程的话，就需要 `400MB` 的内存来存储页表，这是非常大的内存了，更别说 64 位的环境了。



##### 多级页表

把这个 100 多万个「页表项」的单级页表再分页，将页表（一级页表）分为 `1024` 个页表（二级页表），每个表（二级页表）中包含 `1024` 个「页表项」，形成**二级分页**。

<img src="https://img-blog.csdnimg.cn/19296e249b2240c29f9c52be70f611d5.png" alt="img" style="zoom:50%;" />

> 分了二级表，映射 4GB 地址空间就需要 4KB（一级页表）+ 4MB（二级页表）的内存，这样占用空间不是更大了吗？

其实我们应该换个角度来看问题，还记得计算机组成原理里面无处不在的**局部性原理**么？

每个进程都有 4GB 的虚拟地址空间，而显然对于大多数程序来说，其使用到的空间远未达到 4GB，因为会存在部分对应的页表项都是空的，根本没有分配，对于已分配的页表项，如果存在最近一定时间未访问的页表，在物理内存紧张的情况下，操作系统会将页面换出到硬盘，也就是说不会占用物理内存。

如果使用了二级分页，一级页表就可以覆盖整个 4GB 虚拟地址空间，但**如果某个一级页表的页表项没有被用到，也就不需要创建这个页表项对应的二级页表了，即可以在需要时才创建二级页表**。而单级页表无论如何都需要4KB的内存。



**对于 64 位的系统，两级分页肯定不够了，就变成了四级目录，分别是：**

<img src="https://img-blog.csdnimg.cn/98e1b77d34784e18a9d7cc0a57706507.png" alt="img" style="zoom:50%;" />

- 全局页目录项 PGD（*Page Global Directory*）；
- 上层页目录项 PUD（*Page Upper Directory*）；
- 中间页目录项 PMD（*Page Middle Directory*）；
- 页表项 PTE（*Page Table Entry*）；



##### TLB

**将访问过的多级页表项对应的物理地址缓存到TLB中，下一次直接通过页内偏移量就可以找到内容了**

程序是有局部性的，即在一段时间内，整个程序的执行仅限于程序中的某一部分。相应地，执行所访问的存储空间也局限于某个内存区域。

<img src="https://img-blog.csdnimg.cn/a3cdf27646b24614a64cfc5d7ccffa35.png" alt="img" style="zoom:50%;" />

有了 TLB 后，那么 CPU 在寻址时，会先查 TLB，如果没找到，才会继续查常规的页表。

TLB 的命中率其实是很高的，因为程序最常访问的页就那么几个。



### 段页式内存管理

内存分段和内存分页并不是对立的，它们是可以组合起来在同一个系统中使用的，那么组合起来后，称为**段页式内存管理**。

段页式内存管理实现的方式：

**程序划为段 -> 段划为页存储**

- 先将程序划分为多个有逻辑意义的段，也就是前面提到的分段机制；
- 接着再把每个段划分为多个页，也就是对分段划分出来的连续空间，再划分固定大小的页；

<img src="https://img-blog.csdnimg.cn/8904fb89ae0c49c4b0f2f7b5a0a7b099.png" alt="img" style="zoom:50%;" />

段页式地址变换中要得到物理地址须经过**三次内存访问**：

- 第一次访问段表，得到页表起始地址；
- 第二次访问页表，得到物理页号；
- 第三次将物理页号与页内位移组合，得到物理地址。

#### 页表项的字段

页表项通常有如下图的字段：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E9%A1%B5%E8%A1%A8%E9%A1%B9%E5%AD%97%E6%AE%B5.png" alt="img" style="zoom:50%;" />

- ***状态位***：用于表示该页是否有效，也就是说是否在物理内存中，供程序访问时参考。
- ***访问字段***：用于记录该页在一段时间被访问的次数，供页面置换算法选择出页面时参考。
- ***修改位***：表示该页在调入内存后是否有被修改过，由于内存中的每一页都在磁盘上保留一份副本，因此，如果没有修改，在置换该页时就不需要将该页写回到磁盘上，以减少系统的开销；如果已经被修改，则将该页重写到磁盘上，以保证磁盘中所保留的始终是最新的副本。
- ***硬盘地址***：用于指出该页在硬盘上的地址，通常是物理块号，供调入该页时使用。





### Linux内存管理

![img](https://img-blog.csdnimg.cn/bc0aaaf379fc4bc8882efd94b9052b64.png)

Linux 主要使用**分页管理**，但是为了**适配硬件的段页式管理机制**，**把所有段的基地址设为 `0`**，也就意味着所有程序的地址空间都是线性地址空间（虚拟地址），相当于屏蔽了 CPU 逻辑地址的概念，所以段只被用于访问控制和内存保护。

Linux 系统中的每个段都是从 0 地址开始的整个 4GB 虚拟空间（32 位环境下），也就是**所有的段的起始地址都是一样的**。这意味着，Linux 系统中的代码，包括操作系统本身的代码和应用程序代码，**所面对的地址空间都是线性地址空间（虚拟地址）**，这种做法相当于屏蔽了处理器中的逻辑地址概念，段只被用于访问控制和内存保护。

<img src="https://img-blog.csdnimg.cn/3a6cb4e3f27241d3b09b4766bb0b1124.png" alt="img" style="zoom:50%;" />

- `32` 位系统的内核空间占用 `1G`，位于最高处，剩下的 `3G` 是用户空间；
- `64` 位系统的内核空间和用户空间都是 `128T`，分别占据整个内存空间的最高和最低处，剩下的中间部分是未定义的。



> 内核空间与用户空间的区别：

- 进程在用户态时，只能访问用户空间内存；
- 只有进入内核态后，才可以访问内核空间的内存；



<img src="https://img-blog.csdnimg.cn/48403193b7354e618bf336892886bcff.png" alt="img" style="zoom:50%;" />

虽然每个进程都各自有独立的虚拟内存，但是**每个虚拟内存中的内核地址，其实关联的都是相同的物理内存**。这样，进程切换到内核态后，就可以很方便地访问内核空间内存。



<img src="https://img-blog.csdnimg.cn/img_convert/b4f882b9447760ce5321de109276ec23.png" alt="虚拟内存空间划分" style="zoom:50%;" />

从**低到高**分别是 6 种不同的内存段：

- 程序文件段（.text），包括二进制可执行代码；
- 已初始化数据段（.data），包括静态常量；
- 未初始化数据段（.bss），包括未初始化的静态变量；
- 堆段，包括动态分配的内存，从低地址开始向上增长；
- 文件映射段，包括动态库、共享内存等，从低地址开始向上增长（[跟硬件和内核版本有关 (opens new window)](http://lishiwen4.github.io/linux/linux-process-memory-location)）；
- 栈段，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 `8 MB`。当然系统也提供了参数，以便我们自定义大小；

在这 7 个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 `malloc()` 或者 `mmap()` ，就可以分别在堆和文件映射段动态分配内存。



### malloc是怎么分配内存的？

实际上，malloc() 并**不是系统调用**，而是 C 库里的函数，用于动态分配内存。

malloc 申请内存的时候，会有两种方式向操作系统申请堆内存。

- 方式一：通过 brk() 系统调用从堆分配内存
- 方式二：通过 mmap() 系统调用在文件映射区域分配内存；



**1、brk()**（小内存）

通过 brk() 函数将「堆顶」指针向高地址移动，获得新的内存空间

<img src="https://img-blog.csdnimg.cn/img_convert/0dd0e2c1eb32b8b7cabfb95392a36f82.png" alt="图片" style="zoom:50%;" />



**2、mmap()**（大内存）

通过 mmap() 系统调用中「私有匿名映射」的方式，在文件映射区分配一块内存，也就是从文件映射区“偷”了一块内存。

<img src="https://img-blog.csdnimg.cn/img_convert/f8425aa73ca7e5ac8e3a46c2e3eb9188.png" alt="图片" style="zoom:50%;" />



> 什么场景下 malloc() 会通过 brk() 分配内存？又是什么场景下通过 mmap() 分配内存？

malloc() 源码里默认定义了一个阈值：

- 如果用户分配的内存小于 128 KB，则通过 brk() 申请内存；
- 如果用户分配的内存大于 128 KB，则通过 mmap() 申请内存；



### malloc() 分配的是物理内存吗？

不是的，**malloc() 分配的是虚拟内存**。

如果分配后的虚拟内存没有被访问的话，是不会将虚拟内存不会映射到物理内存，这样就不会占用物理内存了。

只有在访问已分配的虚拟地址空间的时候，操作系统通过查找页表，发现虚拟内存对应的页没有在物理内存中，就会触发**缺页中断**，进程会从用户态切换到内核态，然后操作系统会建立虚拟内存和物理内存之间的映射关系。





### 小内存的分配与释放

#### malloc(1) 会分配多大的虚拟内存？

malloc() 在分配内存的时候，并不是老老实实按用户预期申请的字节数来分配内存空间大小，而是**会预分配更大的空间作为内存池**。

具体会预分配多大的空间，跟 malloc 使用的内存管理器有关系，我们就以 malloc 默认的内存管理器（Ptmalloc2）来分析。

堆空间的内存地址范围是 00d73000-00d94000，这个范围大小是 132KB，也就说明了 **malloc(1) 实际上预分配 132K 字节的内存**。



#### free 释放内存，会归还给操作系统吗？

- malloc 通过 **brk()** 方式申请的内存，free 释放内存的时候，**并不会把内存归还给操作系统，而是缓存在 malloc 的内存池中，待下次使用**；
- malloc 通过 **mmap()** 方式申请的内存，free 释放内存的时候，**会把内存归还给操作系统，内存得到真正的释放**。

通过 free 释放内存后，**堆内存**还是存在的，并没有归还给操作系统。

这是因为与其把这 1 字节释放给操作系统，不如先缓存着放进 malloc 的内存池里，当进程再次申请 1 字节的内存时就可以直接复用，这样速度快了很多。



### 为什么不全部mmap或者brk来分配内存

**1、mmap分配内存的缺点**

- 申请内存的操作应该避免**频繁的系统调用**，如果都用 mmap 来分配内存，等于每次都要执行系统调用。
- 因为 mmap 分配的内存每次释放的时候，都会归还给操作系统，于是每次 mmap 分配的虚拟地址都是缺页状态的，然后在第**一次访问该虚拟地址的时候，就会触发缺页中断**。
- 为了改进这两个问题，malloc 通过 ` brk() ` 系统调用在堆空间申请内存的时候，由于堆空间是连续的，所以直接**预分配更大的内存来作为内存池**，当内存释放的时候，就缓存在内存池中。



**2、brk分配内存的缺点**

<img src="https://img-blog.csdnimg.cn/img_convert/75edee0cb75450e7987a8a482b975bda.png" alt="图片" style="zoom:50%;" />

对于小块内存，堆内将产生越来越多不可用的碎片，导致“内存泄露”。而这种“泄露”现象使用 valgrind 是无法检测出来的。



### free()只传入一个内存地址，为什么能知道要释放多大的内存

 malloc 返回给用户态的内存起始地址比进程的堆空间起始地址多 16 字节，用来存放该内存块的描述信息。

<img src="https://img-blog.csdnimg.cn/img_convert/cb6e3ce4532ff0a6bfd60fe3e52a806e.png" alt="图片" style="zoom:50%;" />

这样当执行 free() 函数时，free 会对传入进来的内存地址向左偏移 16 字节，然后从这个 16 字节的分析出当前的内存块的大小，自然就知道要释放多大的内存了。



## 进程管理

### 进程

#### 进程状态

![七种状态变迁](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/10-%E8%BF%9B%E7%A8%8B%E4%B8%83%E4%B8%AD%E7%8A%B6%E6%80%81.jpg)

上图中各个状态的意义：

- 运行状态（*Runing*）：该时刻进程占用 CPU；
- 就绪状态（*Ready*）：可运行，由于其他进程处于运行状态而暂时停止运行；
- 阻塞状态（*Blocked*）：该进程正在等待某一事件发生（如等待输入/输出操作的完成）而暂时停止运行，这时，即使给它CPU控制权，它也无法运行；

- 创建状态（*new*）：进程正在被创建时的状态；
- 结束状态（*Exit*）：进程正在从系统中消失时的状态；

在虚拟内存管理的操作系统中，通常会**把阻塞状态的进程的物理内存空间换出到硬盘**，等需要再次运行的时候，再从硬盘换入到物理内存。这个被换出以后的状态就是挂起状态。

- 阻塞挂起状态：进程在外存（硬盘）并等待某个事件的出现；
- 就绪挂起状态：进程在外存（硬盘），但只要进入内存，即刻立刻运行；



#### 进程的控制结构（PCB）

在操作系统中，是用**进程控制块（*process control block，PCB*）**数据结构来描述进程的。在Linux中，PCB以结构体 `task_struct` 表示。

**PCB 是进程存在的唯一标识**，这意味着一个进程的存在，必然会有一个 PCB，如果进程消失了，那么 PCB 也会随之消失。



> PCB包含什么信息呢？

**1、进程描述信息：**

- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；

**2、进程控制和管理信息**

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**3、资源分配清单**

- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。

**4、CPU相关信息**

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。



> 每个PCB是如何组织的？

通常是通过**链表**的方式进行组织，把具有**相同状态的进程链在一起，组成各种队列**。比如：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/12-PCB%E7%8A%B6%E6%80%81%E9%93%BE%E8%A1%A8%E7%BB%84%E7%BB%87.jpg" alt="就绪队列和阻塞队列" style="zoom:50%;" />

- 将所有处于就绪状态的进程链在一起，称为**就绪队列**；
- 把所有因等待某事件而处于等待状态的进程链在一起就组成各种**阻塞队列**；
- 另外，对于运行队列在单核 CPU 系统中则只有一个运行指针了，因为单核 CPU 在某个时间，只能运行一个程序。



此外还有索引的方式：将同一状态的进程组织在一个索引表中，索引表项指向相应的 PCB，不同状态对应不同的索引表。

<img src="https://img-blog.csdnimg.cn/20200816203851862.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQzNzIyMDc5,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:50%;" />

**一般会选择链表，因为可能面临进程创建，销毁等调度导致进程状态发生变化，所以链表能够更加灵活的插入和删除。**



#### 进程的控制

进程的控制，就是进程**创建、终止、阻塞、唤醒**的过程

**1、创建进程**

操作系统允许一个进程创建另一个进程，而且允许子进程继承父进程所拥有的资源，当子进程被终止时，其在父进程处继承的资源应当还给父进程。同时，终止父进程时同时也会终止其所有的子进程。

创建进程的过程如下：

- 为新进程分配一个唯一的进程标识号，并申请一个空白的 PCB ，**PCB 是有限的，若申请失败则创建失败**；
- 为进程分配资源，此处如果资源不足，进程就会进入等待状态，以等待资源；
- 初始化 PCB；
- 如果进程的调度队列能够接纳新进程，那就将进程插入到就绪队列，等待被调度运行；



**2、终止进程**

进程可以有 3 种终止方式：正常结束、异常结束以及外界干预（信号 `kill` 掉）。

终止进程的过程如下：

- 查找需要终止的进程的 PCB；
- 如果处于执行状态，则立即终止该进程的执行，然后将 CPU 资源分配给其他进程；
- 如果其还有子进程，则应将其所有子进程终止；
- 将该进程所拥有的**全部资源都归还给父进程或操作系统**；
- 将其从 PCB 所在队列中删除；



**3、阻塞进程**

当进程需要等待某一事件完成时，它可以调用阻塞语句把自己阻塞等待。而**一旦被阻塞等待，它只能由另一个进程唤醒**。

阻塞进程的过程如下：

- 找到将要被阻塞进程标识号对应的 PCB；
- 如果该进程为运行状态，则**保护其现场**，将其状态转为阻塞状态，停止运行；
- 将该 PCB 插入到阻塞队列中去；



**4、唤醒进程**

进程由「运行」转变为「阻塞」状态是由于进程必须等待某一事件的完成，所以处于阻塞状态的进程是绝对不可能叫醒自己的。

如果某进程正在等待 I/O 事件，需由别的进程发消息给它，则只有当该进程所期待的事件出现时，才由发现者进程用唤醒语句叫醒它。

唤醒进程的过程如下：

- 在该事件的阻塞队列中找到相应进程的 PCB；
- 将其从阻塞队列中移出，并置其状态为就绪状态；
- 把该 PCB 插入到就绪队列中，等待调度程序调度；



#### 进程的上下文切换

各个进程之间是共享 CPU 资源的，在不同的时候进程之间需要切换，让不同的进程可以在 CPU 执行，那么这个**一个进程切换到另一个进程运行，称为进程的上下文切换**。

> 什么是CPU的上下文切换，与线程的上下文切换有何区别？

CPU 上下文切换就是先把前一个任务的 CPU 上下文（**CPU 寄存器和程序计数器**）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。

然后这里面的「任务」包括，进程、线程和中断。所以，可以根据任务的不同，把 CPU 上下文切换分成：**进程上下文切换、线程上下文切换和中断上下文切换**。



> 进程的上下文切换到底是切换什么？

进程是由内核管理和调度的，所以进程的切换只能发生在**内核态**。

所以，**进程的上下文切换不仅包含了虚拟内存、栈、全局变量等用户空间的资源，还包括了内核堆栈、寄存器等内核空间的资源。**

会把交换的信息保存在进程的 PCB，当要运行另外一个进程的时候，我们需要从这个进程的 PCB 取出上下文



> 什么时候会发生进程的上下文切换

- 为了保证所有进程可以得到公平调度，CPU 时间被划分为一段段的时间片，这些时间片再被轮流分配给各个进程。这样，当**某个进程的时间片耗尽**了，进程就从运行状态变为就绪状态，系统从就绪队列选择另外一个进程运行；
- 进程在**系统资源不足（比如内存不足）**时，要等到资源满足后才可以运行，这个时候进程也会被挂起，并由系统调度其他进程运行；
- 当进程通过睡眠函数 sleep 这样的方法将自己**主动挂起**时，自然也会重新调度；
- 当有**优先级更高的进程运行时**，为了保证高优先级进程的运行，当前进程会被挂起，由高优先级进程来运行；
- 发生**硬件中断**时，CPU 上的进程会被中断挂起，转而执行内核中的中断服务程序；



### 线程

**线程是进程当中的一条执行流程**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/16-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.jpg" alt="多线程" style="zoom:50%;" />

同一个进程内多个线程之间可以**共享代码段、数据段、打开的文件等资源**，但**每个线程各自都有一套独立的寄存器和栈**，这样可以确保线程的控制流是相对独立的。

> 线程的优缺点？

**线程的优点：**

- 一个进程中可以同时存在多个线程；
- 各个线程之间可以并发执行；
- 各个线程之间可以共享地址空间和文件等资源；

**线程的缺点：**

- 当进程中的一个线程崩溃时，会导致其所属进程的所有线程崩溃（这里是针对 C/C++ 语言，Java语言中的线崩溃不会造成进程崩溃）。



#### 线程的上下文切换

操作系统的任务调度，实际上的调度对象是线程，而进程只是给线程提供了虚拟内存、全局变量等资源。

- 当进程只有一个线程时，可以认为进程就等于线程；
- 当进程拥有多个线程时，这些线程会**共享相同的虚拟内存和全局变量**等资源，这些资源在上下文切换时是不需要修改的；



> 线程上下文切换的是什么？

这还得看线程是不是属于同一个进程：

- 当两个线程不是属于同一个进程，则切换的过程就跟进程上下文切换一样；
- **当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据**；

所以，线程的上下文切换相比进程，开销要小很多。



#### 线程的实现

主要有三种线程的实现方式：

- **用户线程（*User Thread*）**：在用户空间实现的线程，不是由内核管理的线程，是由用户态的线程库来完成线程的管理；
- **内核线程（*Kernel Thread*）**：在内核中实现的线程，是由内核管理的线程；
- **轻量级进程（*LightWeight Process*）**：在内核中来支持用户线程；



> 用户线程如何理解？存在什么优势和缺陷？

用户线程是基于用户态的线程管理库来实现的，那么**线程控制块（*Thread Control Block, TCB*）** 也是在库里面来实现的，对于操作系统而言是看不到这个 TCB 的，它只能看到整个进程的 PCB。

**用户线程的整个线程管理和调度，操作系统是不直接参与的，而是由用户级线程库函数来完成线程的管理，包括线程的创建、终止、同步和调度等。**

用户级线程的模型，多个用户线程对应同一个内核线程

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/20-%E7%BA%BF%E7%A8%8BPCB-%E4%B8%80%E5%AF%B9%E5%A4%9A%E5%85%B3%E7%B3%BB.jpg" alt="用户级线程模型" style="zoom: 50%;" />

用户线程的**优点**：

- 每个进程都需要有它私有的线程控制块（TCB）列表，用来跟踪记录它各个线程状态信息（PC、栈指针、寄存器），TCB 由用户级线程库函数来维护，可用于不支持线程技术的操作系统；
- 用户线程的切换也是由线程库函数来完成的，**无需用户态与内核态的切换**，所以速度特别快；

用户线程的**缺点**：

- 由于操作系统不参与线程的调度，如果一个线程发起了系统调用而阻塞，那进程所包含的用户线程都不能执行了。
- 当一个线程开始运行后，除非它主动地交出 CPU 的使用权，否则它所在的进程当中的其他线程无法运行，因为用户态的线程没法打断当前运行中的线程，它没有这个特权，只有操作系统才有，但是用户线程不是由操作系统管理的。
- 由于时间片分配给进程，故与其他进程比，在多线程执行时，**每个线程得到的时间片较少**，执行会比较慢；



> 内核线程如何理解？存在什么优势和缺陷？

**内核线程是由操作系统管理的，线程对应的 TCB 自然是放在操作系统里的，这样线程的创建、终止和管理都是由操作系统负责。**

内核线程的模型，一个用户线程对应一个内核线程

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/21-%E7%BA%BF%E7%A8%8BPCB-%E4%B8%80%E5%AF%B9%E4%B8%80%E5%85%B3%E7%B3%BB.jpg" alt="内核线程模型" style="zoom:50%;" />

内核线程的**优点**：

- 在一个进程当中，如果某个内核线程发起系统调用而被阻塞，并不会影响其他内核线程的运行；
- 分配给线程，多线程的进程获得更多的 CPU 运行时间；

内核线程的**缺点**：

- 在支持内核线程的操作系统中，由内核来维护进程和线程的上下文信息，如 PCB 和 TCB；
- 线程的创建、终止和切换都是通过系统调用的方式来进行，因此对于系统来说，系统开销比较大；



> 轻量级进程如何理解？

**轻量级进程（*Light-weight process，LWP*）是内核支持的用户线程，一个进程可有一个或多个 LWP，每个 LWP 是跟内核线程一对一映射的，也就是 LWP 都是由一个内核线程支持。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/22-LWP.jpg" alt="LWP 模型" style="zoom:50%;" />

LWP 只能由内核管理并像普通进程一样被调度，Linux 内核是支持 LWP 的典型例子。

可以在 LWP 上创建多个用户线程，由用户自己进行调度。



#### 一个进程最多可以创建多少个线程？

这个问题跟两个因素有关：

- **进程的虚拟内存空间上限**，因为创建一个线程，操作系统需要为其分配一个栈空间，如果线程数量越多，所需的栈空间就要越大，那么虚拟内存就会占用的越多。
- **系统参数限制**，虽然 Linux 并没有内核参数来控制单个进程创建的最大线程个数，但是有系统级别的参数来控制整个系统的最大线程个数。

在 32 位 Linux 系统里，一个进程的虚拟空间是 4G，内核分走了1G，**留给用户用的只有 3G**。

那么假设创建一个线程需要占用 10M 虚拟内存（这个可以通过 `ulimit -a` 来看），总共有 3G 虚拟内存可以使用。于是我们可以算出，最多可以创建差不多 300 个（3G/10M）左右的线程。

除了虚拟内存的限制，还有系统的限制。

比如下面这三个内核参数的大小，都会影响创建线程的上限：

- ***/proc/sys/kernel/threads-max***，表示系统支持的最大线程数，默认值是 `14553`；
- ***/proc/sys/kernel/pid_max***，表示系统全局的 PID 号数值的限制，每一个进程或线程都有 ID，ID 的值超过这个数，进程或线程就会创建失败，默认值是 `32768`；
- ***/proc/sys/vm/max_map_count***，表示限制一个进程可以拥有的VMA(虚拟内存区域)的数量，具体什么意思我也没搞清楚，反正如果它的值很小，也会导致创建线程失败，默认值是 `65530`。



### 协程、线程与进程的比较

#### 线程与进程的比较：

- 进程是资源（包括内存、打开的文件等）分配的最小单位，线程是 CPU 调度的最小单位；
- 进程拥有一个完整的资源平台，而线程只独享必不可少的资源，如寄存器和栈；
- 线程的上下文切换更快，进程切换时需要刷新 TLB 并获取新的地址空间，然后切换硬件上下文和内核栈，线程切换时只需要切换硬件上下文和内核栈。
- 线程同样具有就绪、阻塞、执行三种基本状态，同样具有状态之间的转换关系；
- 线程能减少并发执行的时间和空间开销；

对于，**线程相比进程能减少开销**，体现在：

- 线程的创建时间比进程快，因为进程在创建的过程中，还需要资源管理信息，比如内存管理信息、文件管理信息，而线程在创建的过程中，不会涉及这些资源管理信息，而是共享它们；
- 线程的终止时间比进程快，因为**线程释放的资源**相比进程少很多；
- 同一个进程内的线程切换比进程切换快，因为**线程具有相同的地址空间**（虚拟内存共享），这意味着同一个**进程的线程都具有同一个页表，那么在切换的时候不需要切换页表**。而对于进程之间的切换，切换的时候要把页表给切换掉，而页表的切换过程开销是比较大的；
- 由于同一进程的各线程间共享内存和文件资源，那么在线程之间数据传递的时候，就不需要经过内核了，这就使得线程之间的数据交互效率更高了；

所以，不管是时间效率，还是空间效率线程比进程都要高



#### 协程与线程的比较：

协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 

协程与线程主要区别是它将不再被内核调度，而是交给了程序自己而线程是将自己交给内核调度。

（1）协程执行效率极高。协程直接操作栈基本没有内核切换的开销，所以上下文的切换非常快，切换开销比线程更小。

（2）协程不需要多线程的锁机制，因为多个协程从属于一个线程，不存在同时写变量冲突，效率比线程高。

（3）一个线程可以有多个协程。



### 调度

选择一个进程运行这一功能是在操作系统中完成的，通常称为**调度程序**（*scheduler*）



#### 调度时机

在进程的生命周期中，当进程从一个运行状态到另外一状态变化的时候，其实会触发一次调度。

如果硬件时钟提供某个频率的周期性中断，那么可以根据如何处理时钟中断 ，把调度算法分为两类：

- **非抢占式调度算法**挑选一个进程，然后让该进程运行直到被阻塞，或者直到该进程退出，才会调用另外一个进程，也就是说不会理时钟中断这个事情。
- **抢占式调度算法**挑选一个进程，然后让该进程只运行某段时间，如果在该时段结束时，该进程仍然在运行时，则会把它挂起，接着调度程序从就绪队列挑选另外一个进程。这种抢占式调度处理，需要在时间间隔的末端发生**时钟中断**，以便把 CPU 控制返回给调度程序进行调度，也就是常说的**时间片机制**。



#### 调度原则

五个原则：

- **CPU 利用率**：调度程序应确保 CPU 是始终匆忙的状态，这可提高 CPU 的利用率；
- **系统吞吐量**：吞吐量表示的是单位时间内 CPU 完成进程的数量，长作业的进程会占用较长的 CPU 资源，因此会降低吞吐量，相反，短作业的进程会提升系统吞吐量；
- **周转时间**：周转时间是进程运行+阻塞时间+等待时间的总和，一个进程的周转时间越小越好；
- **等待时间**：这个等待时间不是阻塞状态的时间，而是进程处于就绪队列的时间，等待的时间越长，用户越不满意；
- **响应时间**：用户提交请求到系统第一次产生响应所花费的时间，在交互式系统中，响应时间是衡量调度算法好坏的主要标准。



#### 调度算法

##### 单核CPU的调度算法

**1、FCFS（先来先服务）**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/24-%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1.jpg" alt="FCFS 调度算法" style="zoom:50%;" />

每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。

这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

- **FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统**



**2、SJF（最短作业优先）**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/25-%E6%9C%80%E7%9F%AD%E4%BD%9C%E4%B8%9A%E4%BC%98%E5%85%88%E7%AE%97%E6%B3%95.jpg" alt="SJF 调度算法" style="zoom:50%;" />

优先选择运行时间最短的进程来运行，这有助于提高系统的吞吐量。

**对长作业不利**



**3、HRRN（高响应比优先）**

用于权衡短作业和长作业，每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/26-%E5%93%8D%E5%BA%94%E6%AF%94%E5%85%AC%E5%BC%8F.jpg" alt="img" style="zoom:50%;" />

- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；



**4、RR（时间片轮转）**

每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。

- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配给另外一个进程；
- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/27-%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%AF%A2.jpg" alt="RR 调度算法" style="zoom:50%;" />

时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长。将

一般来说，时间片设为 `20ms~50ms` 通常是一个比较合理的折中值。



**5、HPF（最高优先级调度算法）**

从就绪队列中选择最高优先级的进程进行运行，这称为最高优先级（*Highest Priority First，HPF*）调度算法。

进程的优先级可以分为，静态优先级和动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

该算法也有两种处理优先级高的方法，非抢占式和抢占式：

- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

但是依然有缺点，**可能会导致低优先级的进程永远不会运行**。



**6、多级反馈队列**

**多级反馈队列（*Multilevel Feedback Queue*）调度算法**是「时间片轮转算法*RR*」和「最高优先级算法*HPF*」的综合和发展。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/28-%E5%A4%9A%E7%BA%A7%E9%98%9F%E5%88%97.jpg" alt="多级反馈队列" style="zoom:50%;" />

- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。
- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

**工作流程：**

- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，**如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾**，以此类推，直至完成；
- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；



### 进程间通信

- [ ] 管道（两种）->消息队列->共享内存->信号->信号量->socket通信

每个进程的用户地址空间都是独立的，一般而言是不能互相访问的，但内核空间是每个进程都共享的，所以进程之间要通信**必须通过内核**。



#### 管道

分为匿名管道和命名管道两种。

**管道这种通信方式效率低，不适合进程间频繁地交换数据**

管道的生命周期，是随进程的创建而建立，随进程的结束而销毁。



##### 匿名管道

Linux命令行中的「`|`」竖线就是一个**管道**，它的功能是将前一个命令（`ps auxf`）的输出，作为后一个命令（`grep mysql`）的输入

```bash
ps auxf | grep mysql
```

匿名管道的创建，需要通过下面这个系统调用：

```c++
int pipe(int fd[2])
```

这里表示创建一个匿名管道，并返回了两个描述符，一个是管道的读取端描述符 `fd[0]`，另一个是管道的写入端描述符 `fd[1]`。注意，这个匿名管道是特殊的文件，只存在于内存，不存于文件系统中。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/6-%E7%AE%A1%E9%81%93-pipe-fork.jpg" alt="img" style="zoom:50%;" />

**所谓的管道，就是内核里面的一串缓存**。从管道的一段写入的数据，实际上是缓存在内核中的，另一端读取，也就是从内核中读取这段数据。另外，管道传输的数据是无格式的流且大小受限。

管道只能一端写入，另一端读出，所以上面这种模式容易造成混乱，因为父进程和子进程都可以同时写入，也都可以读出。那么，为了避免这种情况，通常的做法是：

- 父进程关闭读取的 fd[0]，只保留写入的 fd[1]；
- 子进程关闭写入的 fd[1]，只保留读取的 fd[0]；

如果需要双向通信，则应该创建两个管道。

**对于匿名管道，它的通信范围是存在父子关系的进程**。因为管道没有实体，也就是没有管道文件，只能通过 fork 来复制父进程 fd 文件描述符，来达到通信的目的。

不管是匿名管道还是命名管道，进程写入的数据都是缓存在内核中，另一个进程读取数据时候自然也是从内核中获取，同时通信数据都遵循**先进先出**原则，不支持 lseek 之类的文件定位操作。



##### 命名管道

在使用命名管道前，先需要通过 `mkfifo` 命令来创建，并且指定管道名字：

```bash
mkfifo myPipe #创建管道
echo "hello" > myPipe #将数据写入管道
cat < myPipe #读取管道内的数据
```

**对于命名管道，它可以在不相关的进程间也能相互通信**。因为命令管道，提前创建了一个类型为管道的设备文件，在进程里只要使用这个设备文件，就可以相互通信。



#### 消息队列

**消息队列是保存在内核中的消息链表**，在发送数据时，会分成一个一个独立的数据单元，也就是消息体（数据块），消息体是用户自定义的数据类型，消息的发送方和接收方要约定好消息体的数据类型，所以每个消息体都是固定大小的存储块，不像管道是无格式的字节流数据。**如果进程从消息队列中读取了消息体，内核就会把这个消息体删除**。

**消息队列生命周期随内核**，如果没有释放消息队列或者没有关闭操作系统，消息队列会一直存在。

但邮件的通信方式存在不足的地方有两点，**一是通信不及时，二是附件也有大小限制**，这同样也是消息队列通信不足的点。

**消息队列通信过程中，存在用户态与内核态之间的数据拷贝开销**，因为进程写入数据到内核中的消息队列时，会发生从用户态拷贝数据到内核态的过程，同理另一进程读取内核中的消息数据时，会发生从内核态拷贝数据到用户态的过程



#### 共享内存

消息队列的读取和写入的过程，都会有发生用户态与内核态之间的消息拷贝过程。那**共享内存**的方式，就很好的解决了这一问题。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/9-%E5%85%B1%E4%BA%AB%E5%86%85%E5%AD%98.jpg" alt="img" style="zoom:50%;" />

**共享内存的机制，就是拿出一块虚拟地址空间来，映射到相同的物理内存中**。这样这个进程写入的东西，另外一个进程马上就能看到了，都不需要拷贝来拷贝去，传来传去，大大提高了进程间通信的速度。

> 共享内存的注意事项

1. 共享内存是效率最高的IPC方式，因为无需进行多次复制，直接对内存操作。
2. 操作系统并不保证任何并发问题，例如两个进程同时更改同一块内存区域，会造成数据丢失的情况。
3. 共享内存映射到每个进程中的地址空间都是不同的（如果进程A将指向了共享内存中的一个指针，通过共享内存分享给进程B，B不能通过这个指针访问共享内存）
4. 进程A将自己的一个指针通过共享内存共享给进程B，这个指针在进程B中可以算作野指针，会造成访问越界。



> 共享内存的两种方式

`mmap` 和 `shmget` ，前者是建立虚拟内存到实际文件的映射；后者是建立物理内存到虚拟内存的映射。

<img src="https://img-blog.csdnimg.cn/20201016163135964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmdyZW5oYWlveWxq,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述" style="zoom:80%;" />

![在这里插入图片描述](https://img-blog.csdnimg.cn/2020101616505514.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dhbmdyZW5oYWlveWxq,size_16,color_FFFFFF,t_70#pic_center)



1. ```c
   void *mmap(void *addr, size_t length, int prot, int flags,
                 int fd, off_t offset);
   ```

2. ```c
   int shmget(key_t key, size_t size, int shmflg);
   void* shmat(int shmid, const void* shmaddr, int shmflg);
   ```

3. **二者的区别**：

   - 使用mmap时，进程A和进程B分别把文件映射到自己的内存地址空间；使用shm时，进程A和进程B分别把同一块共享内存映射到自己的内存空间
   - mmap会把数据同步到文件中（会有缓存机制），shm不会把数据写入文件，这意味着mmap重启不会丢失共享数据。
   - mmap共享数据需要通过文件，io效率要比shm的内存读写肯定要低很多。
   - 内存空间比磁盘空间小很多，所以使用mmap可以共享的数据比shm大很多。

   





#### 信号量

用了共享内存通信方式，带来新的问题，那就是如果多个进程同时修改同一个共享内存，很有可能就冲突了。例如两个进程都同时写一个地址，那先写的那个进程会发现内容被别人覆盖了。

为了防止多进程竞争共享资源，而造成的数据错乱，所以需要保护机制，使得共享的资源，在任意时刻只能被一个进程访问。正好，**信号量**就实现了这一保护机制。

**信号量其实是一个整型的计数器，主要用于实现进程间的互斥与同步，而不是用于缓存进程间通信的数据**。

信号量表示资源的数量，控制信号量的方式有两种**原子操作**：

- 一个是 **P 操作**，这个操作会把信号量减去 1，相减后如果信号量 < 0，则表明资源已被占用，进程需阻塞等待；相减后如果信号量 >= 0，则表明还有资源可使用，进程可正常继续执行。
- 另一个是 **V 操作**，这个操作会把信号量加上 1，相加后如果信号量 <= 0，则表明当前有阻塞中的进程，于是会将该进程唤醒运行；相加后如果信号量 > 0，则表明当前没有阻塞中的进程；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/10-%E4%BF%A1%E5%8F%B7%E9%87%8F-%E4%BA%92%E6%96%A5.jpg" alt="img" style="zoom:50%;" />

信号初始化为 `1`，就代表着是**互斥信号量**，它可以保证共享内存在任何时刻只有一个进程在访问，这就很好的保护了共享内存。

信号初始化为 `0`，就代表着是**同步信号量**，它可以保证进程 A 应在进程 B 之前执行。



#### 信号

上面说的进程间通信，都是常规状态下的工作模式。**对于异常情况下的工作模式，就需要用「信号」的方式来通知进程。**

信号是进程间通信机制中**唯一的异步通信机制**，因为可以在**任何时候发送信号**给某一进程，一旦有信号产生，我们就有下面这几种，用户进程对信号的处理方式。

**1.执行默认操作**。Linux 对每种信号都规定了默认操作，例如，上面列表中的 SIGTERM 信号，就是终止进程的意思。

**2.捕捉信号**。我们可以为信号定义一个信号处理函数。当信号发生时，我们就执行相应的信号处理函数。

**3.忽略信号**。当我们不希望处理某些信号的时候，就可以忽略该信号，不做任何处理。有两个信号是应用进程无法捕捉和忽略的，即 `SIGKILL` 和 `SEGSTOP`，它们用于在任何时候中断或结束某一进程。



#### Socket通信（不同主机上的进程）

管道、消息队列、共享内存、信号量和信号都是在同一台主机上进行进程间通信，那要想**跨网络与不同主机上的进程之间通信，就需要 Socket 通信了。**

实际上，Socket 通信不仅可以跨网络与**不同主机**的进程间通信，还可以在**同主机**上进程间通信。

**Socket的创建：**

```c++
int socket(int domain, int type, int protocal)
```

- domain 参数用来指定协议族，比如 AF_INET 用于 IPV4、AF_INET6 用于 IPV6、AF_LOCAL/AF_UNIX 用于本机；
- type 参数用来指定通信特性，比如 SOCK_STREAM 表示的是字节流，对应 TCP、SOCK_DGRAM 表示的是数据报，对应 UDP、SOCK_RAW 表示的是原始套接字；
- protocal 参数原本是用来指定通信协议的，但现在基本废弃。因为协议已经通过前面两个参数指定完成，protocol 目前一般写成 0 即可；



##### TCP协议通信

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/12-TCP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg" alt="img" style="zoom:50%;" />



##### UDP协议通信

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E9%97%B4%E9%80%9A%E4%BF%A1/13-UDP%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B.jpg" alt="img" style="zoom:50%;" />

UDP 是没有连接的，所以不需要三次握手，也就不需要像 TCP 调用 listen 和 connect，但是 UDP 的交互仍然需要 IP 地址和端口号，因此也需要 bind。

对于 UDP 来说，不需要要维护连接，那么也就没有所谓的发送方和接收方，甚至都不存在客户端和服务端的概念，只要有一个 socket 多台机器就可以任意通信，因此每一个 UDP 的 socket 都需要 bind。

另外，每次通信时，调用 sendto 和 recvfrom，都要传入目标主机的 IP 地址和端口。



##### 针对本地进程通信的Socket

本地 socket 被用于在**同一台主机上进程间通信**的场景：

- 本地 socket 的编程接口和 IPv4 、IPv6 套接字编程接口是一致的，可以支持「字节流」和「数据报」两种协议；
- 本地 socket 的实现效率大大高于 IPv4 和 IPv6 的字节流、数据报 socket 实现；

对于本地字节流 socket，其 socket 类型是 AF_LOCAL 和 SOCK_STREAM。

对于本地数据报 socket，其 socket 类型是 AF_LOCAL 和 SOCK_DGRAM。

本地字节流 socket 和 本地数据报 socket 在 bind 的时候，不像 TCP 和 UDP 要绑定 IP 地址和端口，而是**绑定一个本地文件**，这也就是它们之间的最大区别。



### 进程的同步

#### 互斥的概念

由于多线程执行操作共享变量的这段代码可能会导致竞争状态，因此我们将此段代码称为**临界区（*critical section*），它是访问共享资源的代码片段，一定不能给多线程同时执行。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/10-%E4%B8%B4%E7%95%8C%E5%8C%BA.jpg" alt="互斥" style="zoom:50%;" />

我们希望这段代码是**互斥（*mutualexclusion*）的，也就说保证一个线程在临界区执行时，其他线程应该被阻止进入临界区**，说白了，就是这段代码执行过程中，最多只能出现一个线程。

说一下互斥也并不是只针对多线程。在多进程竞争共享资源的时候，也同样是可以使用互斥的方式来避免资源竞争造成的资源混乱。



#### 同步的概念

互斥解决了并发进程/线程对临界区的使用问题。这种基于临界区控制的交互作用是比较简单的，只要一个进程/线程进入了临界区，其他试图想进入临界区的进程/线程都会被阻塞着，直到第一个进程/线程离开了临界区。

在多线程里，每个线程并不一定是顺序执行的，它们基本是以各自独立的、不可预知的速度向前推进，但有时候我们又希望多个线程能密切合作，以实现一个共同的任务。



#### 互斥与同步的区别

同步与互斥是两种不同的概念：

- 同步就好比：「操作 A 应在操作 B 之前执行」，「操作 C 必须在操作 A 和操作 B 都完成之后才能执行」等；
- 互斥就好比：「操作 A 和操作 B 不能在同一时刻执行」；



#### 互斥与同步的实现与使用

为了实现进程/线程间正确的协作，操作系统必须提供实现进程协作的措施和方法，主要的方法有两种：

- *锁*：加锁、解锁操作；
- *信号量*：P、V 操作；

这两个都可以方便地实现进程/线程互斥，而信号量比锁的功能更强一些，它还可以方便地实现进程/线程同步。

现代 CPU 体系结构提供的特殊**原子操作指令 —— 测试和置位（*Test-and-Set*）指令**。





##### 锁

使用加锁操作和解锁操作可以解决并发线程/进程的互斥问题。

任何想进入临界区的线程，必须先执行加锁操作。若加锁操作顺利通过，则线程可进入临界区；在完成对临界资源的访问后再执行解锁操作，以释放该临界资源。

**1、忙等待锁（自旋锁）**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/14-%E8%87%AA%E6%97%8B%E9%94%81.jpg" alt="img" style="zoom: 50%;" />

当获取不到锁时，线程就会一直 while 循环，不做任何事情，所以就被称为「忙等待锁」，也被称为**自旋锁（*spin lock*）**。



**2、无等待锁**

无等待锁顾明思议就是获取不到锁的时候，不用自旋。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/15-%E6%97%A0%E7%AD%89%E5%BE%85%E9%94%81.jpg" alt="img" style="zoom:50%;" />

既然不想自旋，那当没获取到锁的时候，就把当前线程放入到锁的等待队列，然后执行调度程序，把 CPU 让给其他线程执行。



##### 信号量

信号量是操作系统提供的一种协调共享资源访问的方法。

通常**信号量表示资源的数量**，对应的变量是一个整型（`sem`）变量。

另外，还有**两个原子操作的系统调用函数来控制信号量的**，分别是：

- *P 操作*：将 `sem` 减 `1`，相减后，如果 `sem < 0`，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；
- *V 操作*：将 `sem` 加 `1`，相加后，如果 `sem <= 0`，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；



#### 同步问题/模型

##### 生产者-消费者问题

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/20-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85.jpg" alt="生产者-消费者模型" style="zoom:50%;" />

生产者-消费者问题描述：

- **生产者**在生成数据后，放在一个缓冲区中；
- **消费者**从缓冲区取出数据处理；
- 任何时刻，**只能有一个**生产者或消费者可以访问缓冲区；

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/21-%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

那么我们需要三个信号量，分别是：

- 互斥信号量 `mutex`：用于互斥访问缓冲区，初始化值为 1；
- 资源信号量 `fullBuffers`：用于消费者询问缓冲区是否有数据，有数据则读取数据，初始化值为 0（表明缓冲区一开始为空）；
- 资源信号量 `emptyBuffers`：用于生产者询问缓冲区是否有空位，有空位则生成数据，初始化值为 n （缓冲区大小）；



##### 哲学家就餐问题

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/23-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90%E6%A8%A1%E5%9E%8B.jpg" alt="哲学家就餐的问题" style="zoom:50%;" />

先来看看哲学家就餐的问题描述：

- `5` 个老大哥哲学家，闲着没事做，围绕着一张圆桌吃面；
- 巧就巧在，这个桌子只有 `5` 支叉子，每两个哲学家之间放一支叉子；
- 哲学家围在一起先思考，思考中途饿了就会想进餐；
- **奇葩的是，这些哲学家要两支叉子才愿意吃面，也就是需要拿到左右两边的叉子才进餐**；
- **吃完后，会把两支叉子放回原处，继续思考**；



**1、先拿左边的叉子，再拿右边的叉子**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/24-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/25-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%80%E9%97%AE%E9%A2%98.jpg" alt="方案一的问题" style="zoom:50%;" />

不过，这种解法存在一个极端的问题：**假设五位哲学家同时拿起左边的叉子，桌面上就没有叉子了， 这样就没有人能够拿到他们右边的叉子，也就说每一位哲学家都会在 `P(fork[(i + 1) % N ])` 这条语句阻塞了，很明显这发生了死锁的现象**。



**2、在拿叉子前加互斥锁**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/26-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

上面程序中的互斥信号量的作用就在于，**只要有一个哲学家进入了「临界区」，也就是准备要拿叉子时，其他哲学家都不能动，只有这位哲学家用完叉子了，才能轮到下一个哲学家进餐。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/27-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%BA%8C%E9%97%AE%E9%A2%98.jpg" alt="方案二的问题" style="zoom:50%;" />

虽然能让哲学家们按顺序吃饭，但是每次进餐只能有一位哲学家，而桌面上是有 5 把叉子，按道理是能可以有两个哲学家同时进餐的，所以从效率角度上，这不是最好的解决方案。



**3、给哲学家编号，不同编号的哲学家按照不同的顺序拿叉子**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/28-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

即让偶数编号的哲学家「先拿左边的叉子后拿右边的叉子」，奇数编号的哲学家「先拿右边的叉子后拿左边的叉子」。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/29-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E4%B8%89-%E5%9B%BE%E8%A7%A3.jpg" alt="方案三可解决问题" style="zoom:50%;" />





**4、设置哲学家的状态**

用一个数组 state 来记录每一位哲学家的三个状态，分别是在**进餐状态、思考状态、饥饿状态**（正在试图拿叉子）。进餐完成的哲学家在放回叉子，通知左右两边的哲学家。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/30-%E5%93%B2%E5%AD%A6%E5%AE%B6%E8%BF%9B%E9%A4%90-%E6%96%B9%E6%A1%88%E5%9B%9B%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

那么，一个哲学家只有在两个邻居都没有进餐时，才可以进入进餐状态。



##### 读者-写者问题

「哲学家进餐问题」对于互斥访问有限的竞争问题（如 I/O 设备）一类的建模过程十分有用。

另外，还有个著名的问题是「读者-写者」，它为数据库访问建立了一个模型。

读者只会读取数据，不会修改数据，而写者即可以读也可以修改数据。

读者-写者的问题描述：

- 「读-读」允许：同一时刻，允许多个读者同时读
- 「读-写」互斥：没有写者时读者才能读，没有读者时写者才能写
- 「写-写」互斥：没有其他写者时，写者才能写



**1、读者优先**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/32-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%80%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

使用信号量的方式来尝试解决：

- 信号量 `wMutex`：控制写操作的互斥信号量，初始值为 1 ；
- 读者计数 `rCount`：正在进行读操作的读者个数，初始化为 0；
- 信号量 `rCountMutex`：控制对 rCount 读者计数器的互斥修改，初始值为 1；

只要有读者正在读的状态，后来的读者都可以直接进入，如果读者持续不断进入，则写者会处于饥饿状态。



2、写者优先

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/33-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%BA%8C%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom:50%;" />

- 只要有写者准备要写入，写者应尽快执行写操作，后来的读者就必须阻塞；
- 如果有写者持续不断写入，则读者就处于饥饿；

在方案一的基础上新增如下变量：

- 信号量 `rMutex`：控制读者进入的互斥信号量，初始值为 1；
- 信号量 `wDataMutex`：控制写者写操作的互斥信号量，初始值为 1；
- 写者计数 `wCount`：记录写者数量，初始值为 0；
- 信号量 `wCountMutex`：控制 wCount 互斥修改，初始值为 1；



**3、读写者公平策略**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E4%BA%92%E6%96%A5%E4%B8%8E%E5%90%8C%E6%AD%A5/34-%E8%AF%BB%E8%80%85%E5%86%99%E8%80%85-%E6%96%B9%E6%A1%88%E4%B8%89%E7%A4%BA%E4%BE%8B.jpg" alt="img" style="zoom: 25%;" />





### 死锁

当两个线程为了保护两个不同的共享资源而使用了两个互斥锁，那么这两个互斥锁应用不当的时候，可能会造成**两个线程都在等待对方释放锁**，在没有外力的作用下，这些线程会一直相互等待，就没办法继续运行，这种情况就是发生了**死锁**。



#### 发生条件

死锁只有**同时满足**以下四个条件才会发生：

- 互斥条件；
- 持有并等待条件；
- 不可剥夺条件；
- 环路等待条件；

**1、互斥条件**

互斥条件是指**多个线程不能同时使用同一个资源**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E4%BA%92%E6%96%A5%E6%9D%A1%E4%BB%B6.png" alt="img" style="zoom: 67%;" />

**2、持有并等待条件**

持有并等待条件是指，当线程 A 已经持有了资源 1，又想申请资源 2，而资源 2 已经被线程 C 持有了，所以线程 A 就会处于等待状态，但是**线程 A 在等待资源 2 的同时并不会释放自己已经持有的资源 1**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E6%8C%81%E6%9C%89%E5%B9%B6%E7%AD%89%E5%BE%85%E6%9D%A1%E4%BB%B6.png" alt="img" style="zoom:67%;" />

**3、不可剥夺条件**

不可剥夺条件是指，当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E4%B8%8D%E5%8F%AF%E5%89%A5%E5%A4%BA%E6%9D%A1%E4%BB%B6.png" alt="img" style="zoom:67%;" />

**4、环路等待条件**

环路等待条件指都是，在死锁发生的时候，**两个线程获取资源的顺序构成了环形链**。

比如，线程 A 已经持有资源 2，而想请求资源 1， 线程 B 已经获取了资源 1，而想请求资源 2，这就形成资源请求等待的环形图。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E7%8E%AF%E8%B7%AF%E7%AD%89%E5%BE%85%E6%9D%A1%E4%BB%B6.png" alt="img" style="zoom: 67%;" />



#### 死锁的排查

工具： `pstack` + `gdb`

`pstack` 用于打印正在运行的进程的栈跟踪信息（在一段时间内，多次执行 `pstack` 如果代码栈总是同一个位置，则该段代码大概率出现问题【假死、死锁、死循环等】）

```bash
// gdb 命令
$ gdb -p 87746

// 打印所有的线程信息
(gdb) info thread
  3 Thread 0x7f60a610a700 (LWP 87747)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
  2 Thread 0x7f60a5709700 (LWP 87748)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
* 1 Thread 0x7f60a610c700 (LWP 87746)  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
//最左边的 * 表示 gdb 锁定的线程，切换到第二个线程去查看

// 切换到第2个线程
(gdb) thread 2
[Switching to thread 2 (Thread 0x7f60a5709700 (LWP 87748))]#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0 

// bt 可以打印函数堆栈，却无法看到函数参数，跟 pstack 命令一样 
(gdb) bt
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6

// 打印第三帧信息，每次函数调用都会有压栈的过程，而 frame 则记录栈中的帧信息
(gdb) frame 3
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
27    printf("thread B waiting get ResourceA \n");
28    pthread_mutex_lock(&mutex_A);

// 打印mutex_A的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_A
$1 = {__data = {__lock = 2, __count = 0, __owner = 87747, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\303V\001\000\001", '\000' <repeats 26 times>, __align = 2}

// 打印mutex_B的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_B
$2 = {__data = {__lock = 2, __count = 0, __owner = 87748, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\304V\001\000\001", '\000' <repeats 26 times>, __align = 2}  
```



#### 避免死锁的发生

产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。

那么避免死锁问题就只需要破环其中一个条件就可以，最常见的并且可行的就是**使用资源有序分配法，来破环环路等待条件**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%AD%BB%E9%94%81/%E8%B5%84%E6%BA%90%E6%9C%89%E5%BA%8F%E5%88%86%E9%85%8D.png" alt="img" style="zoom: 80%;" />

**线程 A 和 线程 B 总是以相同的顺序申请自己想要的资源。**



#### 互斥锁、自旋锁、读写锁、乐观锁、悲观锁的选择



##### 互斥锁与自旋锁

最底层的两种就是「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的

**如果能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%94%81/%E4%BA%92%E6%96%A5%E9%94%81%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B.png" alt="img" style="zoom:50%;" />

当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程，也就是线程切换；
- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

**对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。



##### 读写锁

**读写锁适用于能明确区分读操作和写操作的场景**。

读写锁的工作原理是：

- 当「写锁」没有被线程持有时，多个线程能够并发地持有读锁，这大大提高了共享资源的访问效率，因为「读锁」是用于读取共享资源的场景，所以多个线程同时持有读锁也不会破坏共享资源的数据。
- 但是，一旦「写锁」被线程持有后，读线程的获取读锁的操作会被阻塞，而且其他写线程的获取写锁的操作也会被阻塞。

写锁是独占锁，因为任何时刻只能有一个线程持有写锁，类似互斥锁和自旋锁，而读锁是共享锁，因为读锁可以被多个线程同时持有。



##### 乐观锁和悲观锁

互斥锁、自旋锁、读写锁，都是属于悲观锁。

悲观锁做事比较悲观，它认为**多线程同时修改共享资源的概率比较高，于是很容易出现冲突，所以访问共享资源前，先要上锁**。

多线程同时修改共享资源的概率比较低，就可以采用乐观锁。

乐观锁做事比较乐观，它假定冲突的概率很低，它的工作方式是：**先修改完共享资源，再验证这段时间内有没有发生冲突，如果没有其他线程在修改资源，那么操作完成，如果发现有其他线程已经修改过这个资源，就放弃本次操作**。

乐观锁的心态是，不管三七二十一，先改了资源再说。另外，你会发现**乐观锁全程并没有加锁，所以它也叫无锁编程**。

SVN 和 Git 和在线文档也是用了乐观锁的思想，先让用户编辑代码，然后提交的时候，通过版本号来判断是否产生了冲突，发生了冲突的地方，需要我们自己修改后，再重新提交。

乐观锁虽然去除了加锁解锁的操作，但是一旦发生冲突，重试的成本非常高，所以**只有在冲突概率非常低，且加锁成本非常高的场景时，才考虑使用乐观锁。**



##### CAS与乐观锁的关系

乐观锁是先修改同步资源，再验证有没有发生冲突。

悲观锁是修改共享数据前，都要先加锁，防止竞争。

CAS 是乐观锁没错，但是 CAS 和自旋锁不同之处，自旋锁基于 CAS 加了while 或者睡眠 CPU 的操作而产生自旋的效果，加锁失败会忙等待直到拿到锁，自旋锁是要需要事先拿到锁才能修改数据的，所以算悲观锁。



## 调度算法

### 进程调度算法

进程调度算法也称 CPU 调度算法，毕竟进程是由 CPU 调度的。

发生进程调度的场景：

1. 当进程从运行状态转到等待状态；
2. 当进程从运行状态转到就绪状态；
3. 当进程从等待状态转到就绪状态；
4. 当进程从运行状态转到终止状态；

调度算法影响的是等待时间（进程在就绪队列中等待调度的时间总和），而不能影响进程真在使用 CPU 的时间和 I/O 时间。

#### FCFS（先来先服务）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/24-%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1.jpg" alt="FCFS 调度算法" style="zoom:50%;" />

**每次从就绪队列选择最先进入队列的进程，然后一直运行，直到进程退出或被阻塞，才会继续从队列中选择第一个进程接着运行。**

这似乎很公平，但是当一个长作业先运行了，那么后面的短作业等待的时间就会很长，不利于短作业。

FCFS 对长作业有利，适用于 CPU 繁忙型作业的系统，而不适用于 I/O 繁忙型作业的系统。



#### SJF（最短作业优先）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/25-%E6%9C%80%E7%9F%AD%E4%BD%9C%E4%B8%9A%E4%BC%98%E5%85%88%E7%AE%97%E6%B3%95.jpg" alt="SJF 调度算法" style="zoom:50%;" />

会**优先选择运行时间最短的进程来运行**，这有助于提高系统的吞吐量。

一个长作业在就绪队列等待运行，而这个就绪队列有非常多的短作业，那么就会使得长作业不断的往后推，周转时间变长，致使长作业长期不会被运行。



#### HRRN（高响应比优先调度）

权衡了长作业和短作业

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/26-%E5%93%8D%E5%BA%94%E6%AF%94%E5%85%AC%E5%BC%8F.jpg" alt="img" style="zoom:50%;" />

**每次进行进程调度时，先计算「响应比优先级」，然后把「响应比优先级」最高的进程投入运行**

- 如果两个进程的「等待时间」相同时，「要求的服务时间」越短，「响应比」就越高，这样短作业的进程容易被选中运行；
- 如果两个进程「要求的服务时间」相同时，「等待时间」越长，「响应比」就越高，这就兼顾到了长作业进程，因为进程的响应比可以随时间等待的增加而提高，当其等待时间足够长时，其响应比便可以升到很高，从而获得运行的机会；



#### RR（时间片轮转调度算法）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/27-%E6%97%B6%E9%97%B4%E7%89%87%E8%BD%AE%E8%AF%A2.jpg" alt="RR 调度算法" style="zoom:50%;" />

**每个进程被分配一个时间段，称为时间片（*Quantum*），即允许该进程在该时间段中运行。**

- 如果时间片用完，进程还在运行，那么将会把此进程从 CPU 释放出来，并把 CPU 分配另外一个进程；
- 如果该进程在时间片结束前阻塞或结束，则 CPU 立即进行切换；

另外，时间片的长度就是一个很关键的点：

- 如果时间片设得太短会导致过多的进程上下文切换，降低了 CPU 效率；
- 如果设得太长又可能引起对短作业进程的响应时间变长。将

通常时间片设为 `20ms~50ms` 通常是一个比较合理的折中值



#### HPF（最高优先级优先）

进程的优先级可以分为，静态优先级或动态优先级：

- 静态优先级：创建进程时候，就已经确定了优先级了，然后整个运行时间优先级都不会变化；
- 动态优先级：根据进程的动态变化调整优先级，比如如果进程运行时间增加，则降低其优先级，如果进程等待时间（就绪队列的等待时间）增加，则升高其优先级，也就是**随着时间的推移增加等待进程的优先级**。

该算法也有两种处理优先级高的方法，非抢占式和抢占式：

- 非抢占式：当就绪队列中出现优先级高的进程，运行完当前进程，再选择优先级高的进程。
- 抢占式：当就绪队列中出现优先级高的进程，当前进程挂起，调度优先级高的进程运行。

但是依然有缺点，可能会导致低优先级的进程永远不会运行。



#### MFQ（多级反馈队列）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E5%92%8C%E7%BA%BF%E7%A8%8B/28-%E5%A4%9A%E7%BA%A7%E9%98%9F%E5%88%97.jpg" alt="多级反馈队列" style="zoom:50%;" />

- 「多级」表示有多个队列，每个队列优先级从高到低，同时优先级越高时间片越短。
- 「反馈」表示如果有新的进程加入优先级高的队列时，立刻停止当前正在运行的进程，转而去运行优先级高的队列；

工作流程：

- 设置了多个队列，赋予每个队列不同的优先级，每个**队列优先级从高到低**，同时**优先级越高时间片越短**；
- 新的进程会被放入到第一级队列的末尾，按先来先服务的原则排队等待被调度，如果在第一级队列规定的时间片没运行完成，则将其转入到第二级队列的末尾，以此类推，直至完成；
- 当较高优先级的队列为空，才调度较低优先级的队列中的进程运行。如果进程运行时，有新进程进入较高优先级的队列，则停止当前运行的进程并将其移入到原队列末尾，接着让较高优先级的进程运行；

可以发现，对于短作业可能可以在第一级队列很快被处理完。对于长作业，如果在第一级队列处理不完，可以移入下次队列等待被执行，虽然等待的时间变长了，但是运行时间也会更长了，所以该算法很好的**兼顾了长短作业，同时有较好的响应时间。**



### 内存页面置换算法

#### 最佳页面置换算法

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E6%9C%80%E4%BC%98%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="最佳页面置换算法" style="zoom:50%;" />

最佳页面置换算法基本思路是，**置换在「未来」最长时间不访问的页面**。

该算法实现需要计算内存中每个逻辑页面的「下一次」访问时间，然后比较，选择未来最长时间不访问的页面。

但是实际系统中无法实现，因为程序访问页面时是动态的，我们是**无法预知每个页面在「下一次」访问前的等待时间**。

**最佳页面置换算法作用是为了衡量算法的效率**，你的算法效率越接近该算法的效率，那么说明你的算法是高效的。



#### 先进先出置换算法

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/FIFO%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="先进先出置换算法" style="zoom:50%;" />

「先进先出置换」算法的思想：**选择在内存驻留时间最长的页面进行中置换**。



#### LRU（最近最久未使用）

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/LRU%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="最近最久未使用的置换算法" style="zoom:50%;" />

最近最久未使用（*LRU*）的置换算法的基本思路是，发生缺页时，**选择最长时间没有被访问的页面进行置换**，也就是说，该算法假设已经很久没有使用的页面很有可能在未来较长的一段时间内仍然不会被使用。

这种算法近似最优置换算法，最优置换算法是通过**「未来」**的使用情况来推测要淘汰的页面，而 LRU 则是通过**「历史」**的使用情况来推测要淘汰的页面。

LRU 的实现需要在内存中维护一个所有页面的链表，并且在每次访问内存时都必须要更新「整个链表」。在链表中找到一个页面，删除它，然后把它移动到表头是一个非常费时的操作。**开销较大**



#### Clock（时钟页面置换）算法

该算法的思路是，把所有的页面都保存在一个类似钟面的「环形链表」中，一个表针指向最老的页面。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E6%97%B6%E9%92%9F%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.png" alt="时钟页面置换算法" style="zoom:50%;" />

当发生缺页中断时，算法首先检查表针指向的页面：

- 如果它的访问位位是 0 就淘汰该页面，并把新的页面插入这个位置，然后把表针前移一个位置；
- 如果访问位是 1 就清除访问位，并把表针前移一个位置，重复这个过程直到找到了一个访问位为 0 的页面为止；



#### LFU（最不常用）置换算法

**当发生缺页中断时，选择「访问次数」最少的那个页面，并将其淘汰**。

它的实现方式是，对每个页面设置一个「访问计数器」，每当一个页面被访问时，该页面的访问计数器就累加 1。在发生缺页中断时，淘汰计数器值最小的那个页面。

> 缺点

1. **增加了计数器的成本，遍历链表本身就需要耗费时间**

2. **只考虑了频率问题，没考虑时间的问题**

   比如有些页面在过去时间里访问的频率很高，但是现在已经没有访问了，而当前频繁访问的页面由于没有这些页面访问的次数高，在发生缺页中断时，就会可能会误伤当前刚开始频繁访问，但访问次数还不高的页面。（这个可以**定期减少访问的次数**，比如当发生时间中断时，把过去时间访问的页面的访问次数除以 2，也就说，随着时间的流失，以前的高访问次数的页面会慢慢减少，相当于加大了被置换的概率。）



### 磁盘调度算法

磁盘调度算法的目的很简单，就是为了提高磁盘的访问性能，一般是通过优化磁盘的访问请求顺序来做到的。

寻道的时间是磁盘访问最耗时的部分，如果请求顺序优化的得当，必然可以节省一些不必要的寻道时间，从而提高磁盘的访问性能。

#### FCFS（先来先服务）

先到来的请求，先被服务。

对于请求：98，183，37，122，14，124，65，67

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E5%85%88%E6%9D%A5%E5%85%88%E6%9C%8D%E5%8A%A1.png" alt="先来先服务" style="zoom:50%;" />

先来先服务算法总共移动了 `640` 个磁道的距离，这么一看这种算法，比较简单粗暴，但是如果大量进程竞争使用磁盘，请求访问的磁道可能会很分散，那先来先服务算法在性能上就会显得很差，因为寻道时间过长。



#### SSF（最短寻道时间优先）

最短寻道时间优先（*Shortest Seek First，SSF*）算法的工作方式是，优先选择从当前磁头位置所需寻道时间最短的请求，还是以这个序列为例子：

98，183，37，122，14，124，65，67

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%9C%80%E7%9F%AD%E5%AF%BB%E9%81%93%E6%97%B6%E9%97%B4%E4%BC%98%E5%85%88.png" alt="最短寻道时间优先" style="zoom:50%;" />

但这个算法可能存在某些请求的**饥饿**，因为本次例子我们是静态的序列，看不出问题，假设是一个动态的请求，如果后续来的请求都是小于 183 磁道的，那么 183 磁道可能永远不会被响应，于是就产生了饥饿现象，这里**产生饥饿的原因是磁头在一小块区域来回移动**。



#### 扫描算法（电梯算法）

最短寻道时间优先算法会产生饥饿的原因在于：磁头有可能再一个小区域内来回得移动。

为了防止这个问题，可以规定：**磁头在一个方向上移动，访问所有未完成的请求，直到磁头到达该方向上的最后的磁道，才调换方向，这就是扫描（*Scan*）算法**。

对于请求：98，183，37，122，14，124，65，67

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-%E6%89%AB%E6%8F%8F%E7%AE%97%E6%B3%95.png" alt="扫描算法" style="zoom:50%;" />

扫描调度算法性能较好，不会产生饥饿现象，但是存在这样的问题，中间部分的磁道会比较占便宜，**中间部分相比其他部分响应的频率会比较多**，也就是说每个磁道的响应频率存在差异。



#### 循环扫描算法

扫描算法使得每个磁道响应的频率存在差异，那么要优化这个问题的话，可以总是按相同的方向进行扫描，使得每个磁道的响应频率基本一致。

循环扫描（*Circular Scan, CSCAN* ）规定：只有磁头朝某个特定方向移动时，才处理磁道访问请求，而返回时直接快速移动至最靠边缘的磁道，也就是复位磁头，这个过程是很快的，并且**返回中途不处理任何请求**，该算法的特点，就是**磁道只响应一个方向上的请求**。

对于请求：98，183，37，122，14，124，65，67

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%A3%81%E7%9B%98%E8%B0%83%E5%BA%A6-C-SCAN%E7%AE%97%E6%B3%95.png" alt="循环扫描算法" style="zoom:50%;" />

循环扫描算法相比于扫描算法，对于各个位置磁道响应频率相对比较平均。



#### LOOK 与 C-LOOK算法

扫描算法和循环扫描算法，都是磁头移动到磁盘「最始端或最末端」才开始调换方向。

那这其实是可以优化的，优化的思路就是**磁头在移动到「最远的请求」位置，然后立即反向移动。**

那针对 SCAN 算法的优化则叫 LOOK 算法，它的工作方式，磁头在每个方向上仅仅移动到最远的请求位置，然后立即反向移动，而不需要移动到磁盘的最始端或最末端，**反向移动的途中会响应请求**。



### I/O 调度算法

操作系统的通用块层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。



#### 没有调度算法

不对文件系统和应用程序的 I/O 做任何处理，这种算法常用在虚拟机 I/O 中，此时磁盘 I/O 调度算法交由物理机系统负责。



#### FIFO（先入先出）调度算法



#### 完全公平调度算法

大部分系统都把这个算法作为默认的 I/O 调度器，它为每个进程维护了一个 I/O 调度队列，并按照时间片来均匀分布每个进程的 I/O 请求。



#### 优先级调度算法

优先级高的 I/O 请求先发生， 它适用于运行大量进程的系统，像是桌面环境、多媒体应用等。



#### 最终期限调度算法

分别为读、写请求创建了不同的 I/O 队列，这样可以提高机械磁盘的吞吐量，并确保达到最终期限的请求被优先处理，适用于在 I/O 压力比较大的场景，比如数据库等。



## 文件系统

### 文件系统基本组成

文件系统是操作系统中负责管理持久数据的子系统，把用户的文件存到磁盘硬件中，因为即使计算机断电了，磁盘里的数据并不会丢失，所以可以持久化的保存文件。

Linux 文件系统会为每个文件分配两个数据结构：**索引节点（*index node*）和目录项（*directory entry*）**，它们主要用来记录文件的元信息和目录层次结构。

- 索引节点，也就是 *inode*，用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、**数据在磁盘的位置**等。索引节点是文件的**唯一**标识，它们之间一一对应，也同样都会被存储在硬盘中，所以**索引节点同样占用磁盘空间**。
- 目录项，也就是 *dentry* ，用来记录文件的名字、**索引节点指针**以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，**目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存**。

由于**索引节点 *inode* 唯一标识一个文件**，而**目录项记录着文件的名**，所以目录项和索引节点的关系是多对一，也就是说，一个文件可以有多个别字。比如，硬链接的实现就是多个目录项中的索引节点指向同一个文件。

**注意：**目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。



![ltRE1.jpg](https://s1.328888.xyz/2022/05/25/ltRE1.jpg)

索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。

另外，磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。

- *超级块*，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。
- *索引节点区*，用来存储索引节点；
- *数据块区*，用来存储文件或目录数据；

我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：

- 超级块：当文件系统挂载时进入内存；
- 索引节点区：当文件被访问时进入内存；



#### 目录项和目录是一个东西吗？

不是，**目录是个文件**，持久化存储在磁盘，而**目录项是内核一个数据结构**，缓存在内存。

如果查询目录频繁从磁盘读，效率会很低，所以内核会把已经读过的目录用目录项这个数据结构缓存在内存，下次再次读到相同的目录时，只需从内存读就可以，大大提高了文件系统的效率。

注意，目录项这个数据结构不只是表示目录，也是可以表示文件的。



#### 文件读取的最小单位是多少？

磁盘读写的最小单位是**扇区**，扇区的大小只有 `512B` 大小，很明显，如果每次读写都以这么小为单位，那这读写的效率会非常低。

文件系统把多个扇区组成了一个**逻辑块**，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 `4KB`，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。



### 虚拟文件系统

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%99%9A%E6%8B%9F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.png" alt="img" style="zoom:50%;" />

Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类：

- *磁盘的文件系统*，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。
- *内存的文件系统*，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 `/proc` 和 `/sys` 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据。
- *网络的文件系统*，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等。

文件系统首先要先挂载到某个目录才可以正常使用，比如 Linux 系统在启动时，会把文件系统挂载到根目录。



### 文件的使用

![write 的过程](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%86%99%E5%88%B0%E7%A3%81%E7%9B%98%E8%BF%87%E7%A8%8B.png)

我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪呢，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「**文件描述符**」，所以说文件描述符是打开文件的标识。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E6%89%93%E5%BC%80%E8%A1%A8.png" alt="打开文件表" style="zoom:50%;" />

操作系统在打开文件表中维护着打开文件的状态和信息：

- **文件指针**：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的；
- **文件打开计数器**：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目；
- **文件磁盘位置**：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取；
- **访问权限**：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求；



### 文件的存储

文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种：

- 连续空间存放方式
- 非连续空间存放方式

其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。

不同的存储方式，有各自的特点，重点是要分析它们的存储效率和读写性能，接下来分别对每种存储方式说一下。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%E6%96%B9%E5%BC%8F%E6%AF%94%E8%BE%83.png" alt="img" style="zoom:50%;" />





#### 连续空间存放方式

**文件存放在磁盘「连续的」物理空间中**。这种模式下，文件的数据都是紧密相连，**读写效率很高**，因为一次磁盘寻道就可以读出整个文件。

使用连续存放的方式有一个前提，必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。

所以，**文件头里需要指定「起始块的位置」和「长度」**，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F.png" alt="连续空间存放方式" style="zoom: 67%;" />

注意，此处说的文件头，就类似于 Linux 的 inode。

连续空间存放的方式虽然读写效率高，**但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。**



#### 非连续空间存放方式

非连续空间存放方式分为「链表方式」和「索引方式」。

- **链表方式**

链表的方式存放是**离散的，不用连续的**，于是就可以**消除磁盘碎片**，可大大提高磁盘空间的利用率，同时**文件的长度可以动态扩展**。根据实现的方式的不同，链表可分为「**隐式链表**」和「**显式链接**」两种形式。

> 隐式链表

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E9%93%BE%E8%A1%A8%E6%96%B9%E5%BC%8F.png" alt="隐式链表" style="zoom:50%;" />

文件要以「**隐式链表**」的方式存放的话，**实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置**，这样一个数据块连着一个数据块，从链头开始就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。

这里的文件头类似于 Linux 的 inode。

隐式链表的存放方式的**缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间**。隐式链接分配的**稳定性较差**，系统在运行过程中由于软件或者硬件错误**导致链表中的指针丢失或损坏，会导致文件数据的丢失。**



> 显式链接

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E5%88%86%E9%85%8D%E8%A1%A8.png" alt="显式链接" style="zoom:50%;" />

如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「**显式链接**」，它指**把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中**，该表在整个磁盘仅设置一张，**每个表项中存放链接指针，指向下一个数据块号**。

内存中的这样一个表格称为**文件分配表（*File Allocation Table，FAT*）**。

由于查找记录的过程是在内存中进行的，因而不仅显著地**提高了检索速度**，而且**大大减少了访问磁盘的次数**。但也正是整个表都存放在内存中的关系，它的主要的缺点是**不适用于大磁盘**。



- **索引方式**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E8%BF%9E%E7%BB%AD%E7%A9%BA%E9%97%B4%E5%AD%98%E6%94%BE%E6%96%B9%E5%BC%8F-%E7%B4%A2%E5%BC%95%E6%96%B9%E5%BC%8F.png" alt="索引的方式" style="zoom:50%;" />

链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。

索引的实现是为每个文件创建一个「**索引数据块**」，里面存放的是**指向文件数据块的指针列表**，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。

另外，**文件头需要包含指向「索引数据块」的指针**，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。



> 如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？

1. **链表 + 索引的方式**

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%93%BE%E5%BC%8F%E7%B4%A2%E5%BC%95%E5%9D%97.png" alt="链式索引块" style="zoom:50%;" />

   这种组合称为「**链式索引块**」，它的实现方式是**在索引数据块留出一个存放下一个索引数据块的指针**，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。那这种方式也会出现前面提到的链表方式的问题，万一某个指针损坏了，后面的数据也就会无法读取了。

   

2. **索引 + 索引的方式**

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95%E5%9D%97.png" alt="多级索引块" style="zoom:50%;" />

   这种组合称为「**多级索引块**」，实现方式是**通过一个索引块来存放多个索引数据块**，一层套一层索引，像极了俄罗斯套娃是吧。

   

> 索引方式的优点

- 文件的创建、增大、缩小很方便；
- 不会有碎片的问题；
- 支持顺序读写和随机读写；



#### Unix 文件的实现方式

早期 Unix 文件系统是组合了前面的文件存放方式的优点。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/Unix%20%E5%A4%9A%E7%BA%A7%E7%B4%A2%E5%BC%95.png" alt="早期 Unix 文件系统" style="zoom:50%;" />

根据文件的大小，存放的方式会有所变化：

- 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式；
- 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式；
- 如果前面两种方式都不够存放大文件，则采用二级间接索引方式；
- 如果二级间接索引也不够存放大文件，这采用三级间接索引方式；

那么，文件头（*Inode*）就需要包含 13 个指针：

- 10 个指向数据块的指针；
- 第 11 个指向索引块的指针；
- 第 12 个指向二级索引块的指针；
- 第 13 个指向三级索引块的指针；

所以，这种方式能很灵活地支持小文件和大文件的存放：

- 对于小文件使用直接查找的方式可减少索引数据块的开销；
- 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询；

这个方案就用在了 Linux Ext 2/3 文件系统里，虽然解决大文件的存储，但是**对于大文件的访问，需要大量的查询，效率比较低。**

为了解决这个问题，Ext 4 做了一定的改变，具体怎么解决的，本文就不展开了。



### 空闲空间管理

文件系统负责将空闲块分配给文件，因此它必须跟踪磁盘中存在的所有空闲块。

#### 空闲表法

空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E8%A1%A8%E6%B3%95.png" alt="空闲表法" style="zoom:50%;" />

当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。

这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。



#### 空闲链表法

每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A9%BA%E9%97%B2%E5%9D%97%E9%93%BE%E8%A1%A8.png" alt="空闲链表法" style="zoom:50%;" />

当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。

这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。

空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。



#### 位图法

位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。

当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下：

```text
1111110011111110001110110111111100111 ...
```

在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。



### 文件系统的结构

前面提到 Linux 是用**位图**的方式管理空闲空间，用户在**创建一个新文件**时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要**存储数据**时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的（一个数据块的位图最大只能表示 128M 的文件）。

在 Linux 文件系统，把这个结构称为一个**块组**，那么有 N 多的块组，就能够表示 N 大的文件。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%9D%97%E7%BB%84.png)

最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下：

- *超级块*，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。
- *块组描述符*，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。
- *数据位图和 inode 位图*， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。
- *inode 列表*，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。
- *数据块*，包含文件的有用数据。

你可以会发现每个块组里有很多重复的信息，比如**超级块和块组描述符表，这两个都是全局信息，而且非常的重要**，这么做是有两个原因：

- 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。
- 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。

不过，Ext2 的后续版本采用了稀疏技术。该做法是，超级块和块组描述符表不再存储到文件系统的每个块组中，而是只写入到块组 0、块组 1 和其他 ID 可以表示为 3、 5、7 的幂的块组中。



### 目录的存储

基于 Linux 一切皆文件的设计思想，目录其实也是个文件，可以通过 `vim` 打开它，它也有 inode，inode 里面也是指向一些块。

和普通文件不同的是，**普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。**

在目录文件的块中，最简单的保存格式就是**列表**，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%9B%AE%E5%BD%95%E5%93%88%E5%B8%8C%E8%A1%A8.png" alt="目录格式哈希表" style="zoom: 50%;" />

列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。

第一项是「`.`」，表示当前目录，第二项是「`..`」，表示上一级目录，接下来就是一项一项的文件名和 inode。

如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。

于是，保存目录的格式改成**哈希表**，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。

Linux 系统的 ext 文件系统就是采用了哈希表，来保存目录的内容，这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免哈希冲突。

目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，**把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作**，从而降低了磁盘操作次数，提高了文件系统的访问速度。



### 软链接和硬链接

有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过**硬链接（*Hard Link*）** 和**软链接（*Symbolic Link*）** 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。

#### 硬链接

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E7%A1%AC%E9%93%BE%E6%8E%A5-2.png" alt="硬链接" style="zoom: 50%;" />

硬链接是**多个目录项中的「索引节点」指向一个文件**，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以**硬链接是不可用于跨文件系统的**。由于多个目录项都是指向一个 inode，那么**只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。**



#### 软链接

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E8%BD%AF%E9%93%BE%E6%8E%A5.png" alt="软链接" style="zoom:50%;" />

软链接相当于重新创建一个文件，这个文件有**独立的 inode**，但是这个**文件的内容是另外一个文件的路径**，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以**软链接是可以跨文件系统的**，甚至**目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。**



### 文件 I/O

- [ ] 缓冲与非缓冲 I/O + 直接与非直接 I/O ＋ 阻塞与非阻塞 I/O VS 同步与异步 I/O



#### 缓冲与非缓冲 I/O

文件操作的标准库是可以实现数据的缓存，那么**根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O**：

- 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。
- 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。

这里所说的「缓冲」特指标准库内部实现的缓冲。

比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，**减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的**。



#### 直接与非直接 I/O

我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。

那么，**根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O**：

如果在使用文件操作类的系统调用函数时，指定了 `O_DIRECT` 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。

> 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？

以下几种场景会触发内核缓存的数据写入磁盘：

- 在调用 `write` 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上；
- 用户主动调用 `sync` ，内核缓存会刷到磁盘上；
- 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上；
- 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上；



#### 阻塞与非阻塞 I/O VS 同步与异步 I/O

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%90%8C%E6%AD%A5VS%E5%BC%82%E6%AD%A5IO.png" alt="img" style="zoom:50%;" />

- **阻塞 I/O**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%98%BB%E5%A1%9E%20I_O.png" alt="阻塞 I/O" style="zoom:50%;" />

当用户程序执行 `read` ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，`read` 才会返回。

**阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程**。



- **非阻塞 I/O**

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E9%9D%9E%E9%98%BB%E5%A1%9E%20I_O%20.png" alt="非阻塞 I/O" style="zoom:50%;" />

非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，`read` 调用才可以获取到结果。一般结合 `select/poll/epoll` 等 I/O 多路复用技术使用。

**这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。**



- **异步 I/O**

论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是同步调用。因为它们**在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的**，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。

**异步 I/O** 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/%E5%BC%82%E6%AD%A5%20I_O.png" alt="异步 I/O" style="zoom:50%;" />

发起 `aio_read` 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。



## 设备管理

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E9%A9%B1%E5%8A%A8%E7%A8%8B%E5%BA%8F.png" alt="img" style="zoom:50%;" />

### 设备控制器

为了屏蔽设备之间的差异，每个设备都有一个叫**设备控制器（*Device Control*）** 的组件，比如硬盘有硬盘控制器、显示器有视频控制器等。

设备控制器里有芯片，它可执行自己的逻辑，也有自己的寄存器，用来与 CPU 进行通信，比如：

- 通过写入这些寄存器，操作系统可以命令设备发送数据、接收数据、开启或关闭，或者执行某些其他操作。
- 通过读取这些寄存器，操作系统可以了解设备的状态，是否准备好接收一个新的命令等。

设备控制器有三类寄存器，它们分别是**状态寄存器（*Status Register*）**、 **命令寄存器（*Command Register*）**以及**数据寄存器（*Data Register*）**

- *数据寄存器*，CPU 向 I/O 设备写入需要传输的数据，比如要打印的内容是「Hello」，CPU 就要先发送一个 H 字符给到对应的 I/O 设备。
- *命令寄存器*，CPU 发送一个命令，告诉 I/O 设备，要进行输入/输出操作，于是就会交给 I/O 设备去工作，任务完成后，会把状态寄存器里面的状态标记为完成。
- *状态寄存器*，目的是告诉 CPU ，现在已经在工作或工作已经完成，如果已经在工作状态，CPU 再发送数据或者命令过来，都是没有用的，直到前面的工作已经完成，状态寄存标记成已完成，CPU 才能发送下一个字符和命令。

输入输出设备可分为两大类 ：**块设备（*Block Device*）**和 **字符设备（*Character Device*）**。

- *块设备*，把数据存储在固定大小的块中，每个块有自己的地址，硬盘、USB 是常见的块设备。
- *字符设备*，以字符为单位发送或接收一个字符流，字符设备是不可寻址的，也没有任何寻道操作，鼠标是常见的字符设备。

块设备通常传输的数据量会非常大，于是控制器设立了一个可读写的**数据缓冲区**。

- CPU 写入数据到控制器的缓冲区时，当缓冲区的数据囤够了一部分，才会发给设备。
- CPU 从控制器的缓冲区读取数据时，也需要缓冲区囤够了一部分，才拷贝到内存。



>  CPU 是如何与设备的控制寄存器和数据缓冲区进行通信的？

- *端口 I/O*，每个控制寄存器被分配一个 I/O 端口，可以通过特殊的汇编指令操作这些寄存器，比如 `in/out` 类似的指令。
- *内存映射 I/O*，将所有控制寄存器映射到内存空间中，这样就可以像读写内存一样读写数据缓冲区。



### I/O 控制方式

设备控制器的寄存器一般会有状态标记位，用来标识输入或输出操作是否完成。

设备控制器有一个硬件的**中断控制器**，当设备完成任务后触发中断到中断控制器，中断控制器就通知 CPU，一个中断产生了，CPU 需要停下当前手里的事情来处理中断。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/DMA%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86.png" alt="img" style="zoom: 50%;" />

但中断的方式对于频繁读写数据的磁盘，并不友好，这样 CPU 容易经常被打断，会占用 CPU 大量的时间。对于这一类设备的问题的解决方法是使用 **DMA（*Direct Memory Access*）** 功能，它可以使得设备在 CPU 不参与的情况下，能够自行完成把设备 I/O 数据放入到内存。那要实现 DMA 功能要有 「DMA 控制器」硬件的支持。

DMA 的工作方式如下：

- CPU 需对 DMA 控制器下发指令，告诉它想读取多少数据，读完的数据放在内存的某个地方就可以了；
- 接下来，DMA 控制器会向磁盘控制器发出指令，通知它从磁盘读数据到其内部的缓冲区中，接着磁盘控制器将缓冲区的数据传输到内存；
- 当磁盘控制器把数据传输到内存的操作完成后，磁盘控制器在总线上发出一个确认成功的信号到 DMA 控制器；
- DMA 控制器收到信号后，DMA 控制器发中断通知 CPU 指令完成，CPU 就可以直接用内存里面现成的数据了；



### 设备驱动程序

设备驱动程序初始化的时候，要先注册一个该设备的中断处理函数。

在设备完成 CPU 的指令时，会发送中断来通知操作系统，操作系统会根据中断类型调用相应的中断处理程序进行处理。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/%E4%B8%AD%E6%96%AD%E5%B7%A5%E4%BD%9C%E8%BF%87%E7%A8%8B.png" alt="img" style="zoom: 50%;" />

1. 在 I/O 时，设备控制器如果已经准备好数据，则会通过中断控制器向 CPU 发送中断请求；
2. 保护被中断进程的 CPU 上下文；
3. 转入相应的设备中断处理函数；
4. 进行中断处理；
5. 恢复被中断进程的上下文；



### 通用块层

对于块设备，为了减少不同块设备的差异带来的影响，Linux 通过一个统一的**通用块层**，来管理不同的块设备。

通用块层是处于文件系统和磁盘驱动中间的一个块设备抽象层，它主要有两个功能：

- 第一个功能，向上为文件系统和应用程序，提供访问块设备的标准接口，向下把各种不同的磁盘设备抽象为统一的块设备，并在内核层面，提供一个框架来管理这些设备的驱动程序；
- 第二功能，通用层还会给文件系统和应用程序发来的 I/O 请求排队，接着会对队列重新排序、请求合并等方式，也就是 I/O 调度，主要目的是为了提高磁盘读写的效率。

Linux 内存支持 5 种 I/O 调度算法，分别是：

- 没有调度算法
- 先入先出调度算法
- 完全公平调度算法
- 优先级调度
- 最终期限调度算法



### 存储系统 I/O 软件分层

Linux 存储系统的 I/O 由上到下可以分为三个层次，分别是文件系统层、通用块层、设备层。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/I_O%E8%BD%AF%E4%BB%B6%E5%88%86%E5%B1%82.png" alt="img" style="zoom: 67%;" />

这三个层次的作用是：

- 文件系统层，包括虚拟文件系统和其他文件系统的具体实现，它向上为应用程序统一提供了标准的文件访问接口，向下会通过通用块层来存储和管理磁盘数据。
- 通用块层，包括块设备的 I/O 队列和 I/O 调度器，它会对文件系统的 I/O 请求进行排队，再通过 I/O 调度器，选择一个 I/O 发给下一层的设备层。
- 设备层，包括硬件设备、设备控制器和驱动程序，负责最终物理设备的 I/O 操作。

存储系统的 I/O 是整个系统最慢的一个环节，所以 Linux 提供了不少缓存机制来提高 I/O 的效率。

- 为了提高文件访问的效率，会使用**页缓存、索引节点缓存、目录项缓存**等多种缓存机制，目的是为了减少对块设备的直接调用。
- 为了提高块设备的访问效率， 会使用**缓冲区**，来缓存块设备的数据。



## 文件传输

传输文件的时候，要根据文件的大小来使用不同的方式：

- 传输大文件（大于 1G）的时候，使用「异步 I/O + 直接 I/O」；
- 传输小文件（小于 1G）的时候，则使用「零拷贝技术」；



<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/%E4%BC%A0%E7%BB%9F%E6%96%87%E4%BB%B6%E4%BC%A0%E8%BE%93.png" alt="img" style="zoom:50%;" />

要想提高文件传输的性能，就需要减少「用户态与内核态的上下文切换」（也就是减少系统调用的次数）和「内存拷贝」（在文件传输中，用户缓冲区是没有必要存在的）的次数。

### 如何优化小文件传输的性能？

#### 零拷贝

零拷贝技术实现的方式通常有 2 种：

- mmap + write
- sendfile

##### mmap + write

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/mmap%20%2B%20write%20%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:50%;" />

`read()` 系统调用的过程中会把内核缓冲区的数据拷贝到用户的缓冲区里，于是为了减少这一步开销，我们可以用 `mmap()` 替换 `read()` 系统调用函数。

```c++
buf = mmap(file, len);
write(sockfd, buf, len);
```

`mmap()` 系统调用函数会直接把内核缓冲区里的数据「**映射**」到用户空间，这样，操作系统内核与用户空间就不需要再进行任何的数据拷贝操作。

但这还不是最理想的零拷贝，因为仍然需要通过 CPU 把内核缓冲区的数据拷贝到 socket 缓冲区里，而且仍然需要 4 次上下文切换，因为系统调用还是 2 次。



##### sendfile

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-3%E6%AC%A1%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom: 45%;" />

在 Linux 内核版本 2.1 中，提供了一个专门发送文件的系统调用函数 `sendfile()`，函数形式如下：

```c++
#include <sys/socket.h>
ssize_t sendfile(int out_fd, int in_fd, off_t *offset, size_t count);
```



如果网卡支持 SG-DMA（*The Scatter-Gather Direct Memory Access*）技术（和普通的 DMA 有所不同），我们可以进一步减少通过 CPU 把内核缓冲区里的数据拷贝到 socket 缓冲区的过程。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E9%9B%B6%E6%8B%B7%E8%B4%9D/senfile-%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" alt="img" style="zoom:45%;" />

- 第一步，通过 DMA 将磁盘上的数据拷贝到内核缓冲区里；
- 第二步，缓冲区描述符和数据长度传到 socket 缓冲区，这样网卡的 SG-DMA 控制器就可以直接将内核缓存中的数据拷贝到网卡的缓冲区里，此过程不需要将数据从操作系统内核缓冲区拷贝到 socket 缓冲区中，这样就减少了一次数据拷贝；

**全程没有通过 CPU 来搬运数据，所有的数据都是通过 DMA 来进行传输的。**



##### 使用零拷贝技术的项目

Kafka 这个开源项目，就利用了「零拷贝」技术，从而大幅提升了 I/O 的吞吐率，这也是 Kafka 在处理海量数据为什么这么快的原因之一。



#### PageCache

文件传输的第一步都是先需要先把磁盘文件数据拷贝「内核缓冲区」里，这个「内核缓冲区」实际上是**磁盘高速缓存（*PageCache*），当然在内存中**。

PageCache 的优点主要是两个：

- 缓存最近被访问的数据；
- 预读功能；

程序运行的时候，具有「局部性」，所以通常，刚被访问的数据在短时间内再次被访问的概率很高，于是我们可以用 **PageCache 来缓存最近被访问的数据**，当空间不足时淘汰最久未被访问的缓存。

读取磁盘数据的时候，需要找到数据所在的位置，但是对于机械磁盘来说，就是通过磁头旋转到数据所在的扇区，再开始「顺序」读取数据，但是旋转磁头这个物理动作是非常耗时的，为了降低它的影响，**PageCache 使用了「预读功能」**。

针对大文件的传输，不应该使用 PageCache，也就是说不应该使用零拷贝技术，因为可能由于 PageCache 被大文件占据，而导致「热点」小文件无法利用到 PageCache，这样在高并发的环境下，会带来严重的性能问题。



### 大文件传输用什么方式实现？

**在高并发的场景下，针对大文件的传输的方式，应该使用「异步 I/O + 直接 I/O」来替代零拷贝技术**。







## 进程

### 地址空间分布

<img src="https://s1.328888.xyz/2022/05/08/4dbId.png" style="zoom:80%;" />

从高地址到低地址，一个程序由命令行参数和环境变量、栈、文件映射区、堆、BSS段、数据段、代码段组成。

**（1）命令行参数和环境变量**

命令行参数是指从命令行执行程序的时候，给程序的参数。

**（2）栈区**

存储局部变量、函数参数值。栈从高地址向低地址增长。是一块连续的空间。

**（3）文件映射区**

主要为动态库，将映射到内存的文件内容`mmap()`

**（4）堆区**

动态申请内存用。堆从低地址向高地址增长。

**（5）BSS段**

存放程序中未初始化的全局变量和静态变量的一块内存区域。

**（6）数据段**

存放程序中已初始化的全局变量和静态变量的一块内存区域。

**（7）代码段**

存放程序执行代码的一块内存区域。只读，代码段的头部还会包含一些只读的常数变量。



### 基础概念

**1、定义**

​	编译的代码可执行文件只是储存在硬盘的静态文件，运行时被加载到内存，CPU执行内存中指令，这个运行的程序被称为进程。进程是对运行时程序的封装，操作系统进行资源调度和分配的基本单位。

**2、进程的实现**

​	中断发生后操作系统底层的工作步骤

1. 硬件压入堆栈程序计数器等
2. 硬件从中断向量装入新的程序计数器
3. 保存寄存器的值
4. 设置新的堆栈
5. C中断服务例程运行（典型地读和缓冲输入）
6. 调度程序决定下一个将运行的进程
7. C过程返回至汇编代码
8. 汇编语言过程开始运行新的当前进程

**3、进程表：**

​	为了实现进程模型，操作系统维护着一张表格（一个结构数组），即进程表。

<img src="https://s1.328888.xyz/2022/05/04/hTEfe.jpg" style="zoom: 50%;" />

​	每个进程占有一个进程表项（PCB）

​	该表项包含了一个进程状态的重要信息。

​	包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、账号的调度信息，以及其他在进程由运行态转换到就绪态或阻塞态时必须保存的信息，从而**保证该进程随后能再次启动**，就像从未中断过一样

**4、并发与并行**

1. 单个核心在很短时间内分别执行多个进程，称为并发
2. 多个核心同时执行多个进程称为并行
3. 对于并发来说，CPU需要从一个进程切换到另一个进程，这个过程需要保存进程的状态信息

**5、进程的状态**

​	某个进程在某个时刻所处的状态分为以下几种，运行态、就绪态、阻塞态。对于阻塞状态：

​	比如read系统调用阻塞，进程会占用内存空间，这是一种浪费行为，于是操作系统会有跟内存管理中物理页置换到磁盘一样的行为，把阻塞的进程置换到磁盘中，此时进程未占用物理内存，我们称之为挂起：

​	挂起不仅仅可能是物理内存不足，比如sleep系统调用过着用户执行Ctrl+Z也可能导致挂起。

<img src="https://s1.328888.xyz/2022/05/04/hTxFq.png" style="zoom: 80%;" />

​	**特点：**

1. 就绪态和运行态可以相互转换，其它的都是单向转换。就绪态的进程通过调度算法从而获得CPU时间，转为运行状态；
2. 运行态的进程，在分配给它的CPU时间片用完之后就会转为就绪状态，等待下一次调度。
3. 阻塞态是缺少需要的资源从而由运行态转换而来，但是该资源不包括CPU时间，缺少CPU时间会从运行态转换为就绪态。

**6、进程控制块（PCB）**

操作系统对进程的感知，是通过进程控制块PCB数据结构来描述的。它是进程存在的唯一标识，其包括以下信息：

```c
struct task_struct{
  ...
  // 进程标识符
  ...
  // 上下文信息
  ...
  // 进程状态
  ...
  // 进程优先级
  ...
  // 进程通信有关信息 
  ...
  // 时间和定时器有关信息
  ...
  // 文件系统信息
  ...
  // 虚拟内存信息
  ...
  // 其它
  ...
};
```

1. **进程描述信息：**进程标识符、用户标识符等；
2. **进程控制和管理信息：**进程状态，进程优先级等；
3. **进程资源分配清单：**虚拟内存地址空间信息，打开文件列表，IO设备信息等;
4. **CPU相关信息：**当进程切换时，CPU寄存器的值都被保存在相应PCB中，以便CPU重新执行该进程时能从断点处继续执行：

PCB通过链表形式组织起来，比如有就绪队列、阻塞队列等，方便增删，方便进程管理。

**7、进程切换为何比线程慢**



## 线程与进程

### 进程和线程的区别，什么时候用多线程或者多进程

[参考答案](https://blog.csdn.net/lishenglong666/article/details/8557215)

1. 定义

   进程是资源分配的最小单位，线程是CPU调度的最小单位。
   ![rN2i7.png](https://s1.328888.xyz/2022/04/21/rN2i7.png "进程和线程的区别")

2. 多进程多线程应用场景

* **需要频繁创建销毁的优先用线程**

  原因请看上面的对比。    
  这种原则最常见的应用就是Web服务器了，来一个连接建立一个线程，断了就销毁线程，要是用进程，创建和销毁的代价是很难承受的。

* **需要进行大量计算的优先使用线程**

  所谓大量计算，当然就是要耗费很多CPU，切换频繁了，这种情况下线程是最合适的。这种原则最常见的是图像处理、算法处理。

* **强相关的处理用线程，弱相关的处理用进程**

  什么叫强相关、弱相关？理论上很难定义，给个简单的例子就明白了。
  一般的Server需要完成如下任务：消息收发、消息处理。“消息收发”和“消息处理”就是弱相关的任务，而“消息处理”里面可能又分为“消息解码”、“业务处理”，这两个任务相对来说相关性就要强多了。任务间相关性比较强的用多线程，相关性比较弱的用多进程。因为线程之间的数据共享和同 步比较简单。

* **可能要扩展到多机分布的用进程，多核分布的用线程**

  原因请看上面对比。

* **都满足需求的情况下，用你最熟悉、最拿手的方式**

  至于“数据共享、同步”、“编程、调试”、“可靠性”这几个维度的所谓的“复杂、简单”应该怎么取舍，我只能说：没有明确的选择方法。但我可以告诉你一个选择原则：如果多进程和多线程都能够满足要求，那么选择你最熟悉、最拿手的那个。



### 进程切换为何比线程慢

涉及到**虚拟内存**的问题，进程切换涉及虚拟地址空间的切换而线程不会。

因为**每个进程都有自己的虚拟地址空间**，而**线程是共享所在进程的虚拟地址空间的**，所以同一个进程中的线程进行线程切换时不涉及虚拟地址空间的转换。

把虚拟地址转换为物理地址需要查找页表，页表查找是一个很慢的过程（至少访问2次内存），因此通常使用Cache：来缓存常用的地址映射，这样可以加速页表查找，这个cache就是TLB（快表）



### 单核CPU使用多线程是否能够提高效率？

> 首先思考一下为什么引用多线程？

假设有这么一个场景，程序只有一个单线程程序，当线程执行到某一步，需要等待IO资源或者其他资源时，程序将阻塞在原地，此时CPU将空转，这样就浪费了CPU的资源。

但是如果是多线程，此时可以调度另外一个线程执行其他操作，这样就避免了CPU的空转，提高了CPU的利用率，也减少程序整体的运行时间。

所以：

1. 对于CPU密集型的程序，多线程并不能提高运行效率
2. 对于IO密集型的程序，多线程可以提高运行效率





### 进程调度算法

### 避免死锁的算法

* 银行家算法







# OS 常见面试题



## 整体流程相关

#### 键盘敲入字母时，期间发生了什么？

- [ ] 键盘控制器（发起中断） -> CPU调用键盘的中断处理程序（when to 注册） -> 放入读缓存区队列 -> 等待屏幕定时调用的自己的中断处理程序（将读buffer 的数据转移到写buffer） -> 将写buffer的数据发送给显式控制器

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA/CPU%20%E7%A1%AC%E4%BB%B6%E6%80%BB%E7%BA%BF%E5%9B%BE.png" alt="CPU 的硬件架构图" style="zoom: 50%;" />

CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I/O 桥接器，这个 I/O 桥接器，另一边接入了内存总线，使得 CPU 和内存通信。再另一边，又接入了一个 I/O 总线，用来连接 I/O 设备，比如键盘、显示器等。



那当用户输入了键盘字符，**键盘控制器**就会产生扫描码数据，并将其缓冲在键盘控制器的寄存器中，紧接着键盘控制器通过总线给 CPU 发送**中断请求**。

CPU 收到中断请求后，操作系统会**保存被中断进程的 CPU 上下文**，然后调用键盘的**中断处理程序**。

键盘的中断处理程序是在**键盘驱动程序**初始化时注册的，那键盘**中断处理函数**的功能就是从键盘控制器的寄存器的缓冲区读取扫描码，再根据扫描码找到用户在键盘输入的字符，如果输入的字符是显示字符，那就会把扫描码翻译成对应显示字符的 ASCII 码，比如用户在键盘输入的是字母 A，是显示字符，于是就会把扫描码翻译成 A 字符的 ASCII 码。

得到了显示字符的 ASCII 码后，就会把 ASCII 码放到「读缓冲区队列」，接下来就是要把显示字符显示屏幕了，**显示设备的驱动程序会定时从「读缓冲区队列」读取数据放到「写缓冲区队列」**，最后把「写缓冲区队列」的数据一个一个写入到显示设备的控制器的寄存器中的数据缓冲区，最后将这些数据显示在屏幕里。

显示出结果后，**恢复被中断进程的上下文**。



#### linux 下进程的创建过程分析

| **系统调用** |                           **描述**                           |
| ------------ | :----------------------------------------------------------: |
| fork         | fork创造的子进程是父进程的完整副本，复制了父亲进程的资源，包括内存的内容task_struct内容 |
| vfork        | vfork创建的子进程与父进程共享数据段,而且由vfork()创建的子进程将先于父进程运行 |
| clone        | Linux上创建线程一般使用的是pthread库 实际上linux也给我们提供了创建线程的系统调用，就是clone |

1. 使用系统调用Sys_clone(或fork,vfork)系统调用创建一个新进程，而且都是通过调用do_fork来实现进程的创建；
2. Linux通过复制父进程 PCB 的 task_struct 来创建一个新进程，要给新进程分配一个新的内核堆栈；
3. 要修改复制过来的进程数据，比如pid、进程链表等等执行 copy_process 和 copy_thread；
4. 调度到子进程时的内核栈顶
5. 调度到子进程时的第一条指令地址





#### linux启动过程中，创建了哪些进程？

0号进程 -> 1号内核进程 -> 1号用户进程(init进程) -> getty进程 -> shell进程 -> 命令行执行进程。

所以在命令行中通过 ./program 执行可执行文件时，所有创建的进程都是shell进程的子进程，这也就是为什么shell一关闭，在shell中执行的进程都自动被关闭。



#### 讲一讲程序从输入到执行的全过程？（百度一面）

初始时， shell 程序执行它的指令，等待用户输入一个命令。当用户在键盘输入字符串 "./hello" 后，shell 的程序将字符逐一读入寄存器，再将它存放到内存中。

当用户在键盘上敲回车键时，shell 程序就知道我们已经结束了命令的输人。然后 shell 执行一系列指令来加载可执行的 hello 文件，这些指令将 hello 目标文件中的代码和数据从磁盘复制到内存。数据包括最终会被输出的字符串 "hello, world\n" 。

一旦目标文件 hello 中的代码和数据被加载到内存，处理器就开始执行 hello 程序 的 main 程序中的机器语言指令。这些指令将  "hello, world\n" 字符串中的字节从主存复制到寄存器文件，再从寄存器文件中复制到显示设备，最终显示在屏幕上。



#### 说一下程序从磁盘中到内存进程中运行的过程

1. 在程序运行之前，装载器会进行程序的**装载**，也就是创建一个**进程结构**，它会有自己的一套虚拟地址、页表等结构。但是装载器不会把代码装载到物理内存中，而是用一个**页表**把代码在硬盘上的位置记录下来，只有在真正运行的时候才会加载到内存里面。
2. 装载器会找到程序的入口地址，执行的时候，从入口地址开始读第一条指令。操作系统进行进程的调度，当轮到这个进程来的时候，才从装载器返回的入口点开始执行。
3. CPU从程序入口处取出指令，但是这是一个虚拟地址，需要转换为物理地址。那么怎么转换呢？CPU会去查看页表，可以这个页表现在还指向的是硬盘中的地址，所以CPU会执行缺页中断处理程序最后CPU会从硬盘里面把代码加载入内存，之后CPU当然得把页表修改一下，这样才能反映数据已经进入内存呢。
4. 最后指令装载到内存后，CPU就可以按序执行了。



### 进程/线程相关

#### fork()具体做了什么事？

1. fork()是一个系统调用，会陷入内核态
2. 在内核态中，内核会创建一个新的PCB，在Linux中就是一个 `task_struct` 结构体，然后将父进程 `task_struct` 的资源（页表、文件资源、当前路径、信号、堆栈、寄存器等）完全复制给子进程的 `task_struct` ，这里复制指的是写时复制，内核会把资源对应的页表项设置为只读模式，此时内核是没有将物理内容真正的复制。
3. 只有当其中一个进程对资源进行了写操作，由于页表被设置为只读模式，那么此时会引发缺页中断，使得内核将共享的资源另外复制一份，然后中断返回后重新执行进程的写操作。

> fork采用了这种写时复制的机制，那么fork出来子进程后，理论上子进程和父进程那个先调度呢？

一般来说是子进程优先调度。因为很多情况下，子进程被fork出来以后，就会马上执行exec，去执行另外的程序，此时会清空堆栈等一些和父进程共享的资源，加载新的代码段。如果此时是父进程先调度的话，在父进程中执行了写操作后，会触发写时复制，而子进程并不需要用到属于自己的资源，造成资源和效率浪费。所以内核一般会优先将子进程加入调度队列中。



> 和vfork的区别

1. fork 是将父进程的所有资源的复制一份，而 vfork 是将父进程除虚拟内存以外的资源都复制一份。
2. vfork 结束后父进程和子进程是共享一个地址空间的。而 fork 结束后父进程和子进程拥有自己独立的地址空间。
3. 并且 vfork 会阻塞父进程，直到子进程调用 exec 或者 exit，父进程才会得以执行



> 和 clone 的区别

1. clone可以由用户决定复制父进程的哪些资源，共享父进程的哪些资源。
2. clone不再复制父进程的栈空间，而是自己创建一个新的栈。如果选择和父进程共享一个虚拟地址空间，那么此时创建的就是线程。



## 内存管理相关

#### 程序访问内存的整体流程是怎么样的？

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E6%B5%81%E7%A8%8B.png" alt="虚拟内存的流程" style="zoom:50%;" />

1. 判断内存访问是否越界（页号是否超出页表长度）
2. 看下 TLB 中是否缓存了虚拟地址到物理地址的映射
   - TLB 命中：修改访问位和修改位，返回物理地址
   - TLB 未命中：去内存中找
3. 看下内存中是否缓存了虚拟地址到物理地址的映射
   - 内存命中：更新 TLB ，修改访问位和修改位，返回物理地址
   - 内存未命中：触发缺页中断，由 OS 进行页面置换
4. OS 执行完缺页中断后，该物理地址的内容已经在内存中，因此更新 TLB ，修改访问位和修改位，返回物理地址



#### 缺页中断的流程？

> 缺页中断与一般中断的区别？

当 CPU 访问的页面不在物理内存时，便会产生一个缺页中断，请求操作系统将所缺页调入到物理内存。那它与一般中断的主要区别在于：

- 缺页中断在指令执行**「期间」**产生和处理中断信号，而一般中断在一条指令执行**「完成」后**检查和处理中断信号。
- 缺页中断返回到该指令的开始重新执行**「该指令」**，而一般中断返回回到该指令的**「下一个指令」**执行。



> 缺页异常和缺页中断？

1. **缺页异常**：malloc和mmap函数在分配内存时只是建立了进程虚拟地址空间，并没有分配虚拟内存对应的物理内存。当进程访问这些没有建立映射关系的虚拟内存时，会自动触发一个**缺页异常，引发缺页中断**。
2. **缺页中断**：缺页异常后将产生一个缺页中断，此时操作系统会根据页表中的**外存地址**在外存中找到所缺的一页，将其调入**内存**。



> 具体流程？

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost2/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95/%E7%BC%BA%E9%A1%B5%E5%BC%82%E5%B8%B8%E6%B5%81%E7%A8%8B.png" alt="缺页中断的处理流程" style="zoom:50%;" />

1. 在 CPU 里访问一条 Load M 指令，然后 CPU 会去找 M 所对应的页表项。
2. 如果该页表项的状态位是「有效的」，那 CPU 就可以直接去访问物理内存了，如果状态位是「无效的」，则 CPU 则会发送缺页中断请求。
3. 操作系统收到了缺页中断，则会执行缺页中断处理函数，先会查找该页面在磁盘中的页面的位置（页表中有该页的硬盘地址）。
4. 找到磁盘中对应的页面后，需要把该页面换入到物理内存中，但是在换入前，需要在物理内存中找空闲页，如果找到空闲页，就把页面换入到物理内存中。
5. 页面从磁盘换入到物理内存完成后，则把页表项中的状态位修改为「有效的」。
6. 最后，CPU 重新执行导致缺页异常的指令。

找不到空闲页的话，就说明此时内存已满了，这时候，就需要「页面置换算法」选择一个物理页，如果该物理页有被修改过（脏页），则把它换出到磁盘，然后把该被置换出去的页表项的状态改成「无效的」，最后把正在访问的页面装入到这个物理页中。





#### 内存分配的过程是怎么样的？（腾讯一面）

应用程序通过 malloc 函数申请内存的时候，实际上申请的是虚拟内存，此时并不会分配物理内存。

<img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZclL17ouOHwX7evMsibhcg7XaeyZmRScQRicndIdHic4MCk8ydOmYFnbs9GlADQ4NyR4tlibmXjWaSR4A/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom:50%;" />

只有当应用程序读写了这块虚拟内存，CPU 就会去访问这个虚拟内存， 这时会发现这个虚拟内存没有映射到物理内存， CPU 就会产生**缺页中断**，进程会从用户态切换到内核态，并将缺页中断交给内核的 Page Fault Handler （缺页中断函数）处理。

缺页中断处理函数会看是否有空闲的物理内存，如果有，就直接分配物理内存，并建立虚拟内存与物理内存之间的映射关系。

如果没有空闲的物理内存，那么内核就会开始进行**回收内存**的工作，回收的方式主要是两种：直接内存回收和后台内存回收。

- **后台内存回收**（kswapd）：在物理内存紧张的时候，会唤醒 kswapd 内核线程来回收内存，这个回收内存的过程**异步**的，不会阻塞进程的执行。
- **直接内存回收**（direct reclaim）：如果后台异步回收跟不上进程内存申请的速度，就会开始直接回收，这个回收内存的过程是**同步**的，会阻塞进程的执行。

如果直接内存回收后，空闲的物理内存仍然无法满足此次物理内存的申请，那么内核就会放最后的大招了 ——**触发 OOM （Out of Memory）机制**。

OOM Killer 机制会根据算法选择一个占用物理内存较高的进程，然后将其杀死，以便释放内存资源，如果物理内存依然不足，OOM Killer 会继续杀死占用物理内存较高的进程，直到释放足够的内存位置。



#### 讲一下内存回收中，具体回收的是哪些内存？

主要有两类内存可以被回收，而且它们的回收方式也不同。

- **文件页**（File-backed Page）：内核缓存的磁盘数据（Buffer）和内核缓存的文件数据（Cache）都叫作文件页。大部分文件页，都可以直接释放内存，以后有需要时，再从磁盘重新读取就可以了。而那些被应用程序修改过，并且暂时还没写入磁盘的数据（也就是脏页），就得先写入磁盘，然后才能进行内存释放。所以，**回收干净页的方式是直接释放内存，回收脏页的方式是先写回磁盘后再释放内存**。
- **匿名页**（Anonymous Page）：应用程序通过 mmap 动态分配的堆内存叫作匿名页，这部分内存很可能还要再次被访问，所以不能直接释放内存，它们**回收的方式是通过 Linux 的 Swap 机制**，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。

文件页和匿名页的回收都是基于 LRU 算法，也就是优先回收不常访问的内存。LRU 回收算法，实际上维护着 active 和 inactive 两个双向链表，其中：

- **active_list** 活跃内存页链表，这里存放的是最近被访问过（活跃）的内存页；
- **inactive_list** 不活跃内存页链表，这里存放的是很少被访问（非活跃）的内存页；

越接近链表尾部，就表示内存页越不常访问。这样，在回收内存时，系统就可以根据活跃程度，优先回收不活跃的内存。



#### 内存回收带来的性能影响和解决方式？

> 性能影响分析

内存回收有两种方式。

- 一种是后台内存回收，也就是唤醒 kswapd 内核线程，这种方式是异步回收的，不会阻塞进程。
- 一种是直接内存回收，这种方式是同步回收的，会阻塞进程，这样就会造成很长时间的延迟，以及系统的 CPU 利用率会升高，最终引起系统负荷飙高。

可被回收的内存类型有文件页和匿名页：

- 文件页的回收：对于干净页是直接释放内存，这个操作不会影响性能，而对于脏页会先写回到磁盘再释放内存，这个操作会发生磁盘 I/O 的，这个操作是会影响系统性能的。
- 匿名页的回收：如果开启了 Swap 机制，那么 Swap 机制会将不常访问的匿名页换出到磁盘中，下次访问时，再从磁盘换入到内存中，这个操作是会影响系统性能的。

**回收内存的操作基本都会发生磁盘 I/O 的**，如果回收内存的操作很频繁，意味着磁盘 I/O 次数会很多，这个过程势必会影响系统的性能，整个系统给人的感觉就是很卡。

> 提高性能的方式

- **调整文件页和匿名页的回收倾向**

  从文件页和匿名页的回收操作来看，文件页的回收操作对系统的影响相比匿名页的回收操作会少一点，因为文件页对于干净页回收是不会发生磁盘 I/O 的，而匿名页的 Swap 换入换出这两个操作都会发生磁盘 I/O。

  所以可以更倾向回收文件页的内存，在内核中有一个 `swappiness` 来调整这个倾向程度。

- **尽早触发 kswapd 内核线程异步回收内存**

  内核定义了三个内存阈值（watermark，也称为水位），用来衡量当前剩余内存（pages_free）是否充裕或者紧张，分别是：

  <img src="https://mmbiz.qpic.cn/mmbiz_png/J0g14CUwaZclL17ouOHwX7evMsibhcg7XxO0jqBuqNFXBee73YburLnYpRyNQdNN11R88N9HBwklZzEHb52C8fQ/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1" alt="图片" style="zoom: 67%;" />

  - 页最小阈值（pages_min）；
  - 页低阈值（pages_low）；
  - 页高阈值（pages_high）；

  在剩余内存处于页低阈值和页最小阈值之间时， **kswapd0 会执行内存回收，直到剩余内存大于高阈值（pages_high）为止**。

  当剩余内存小于页最小阈值时，说明用户可用内存基本耗尽，此时会**触发直接内存回收**，阻塞应用进程。

  可以通过调节这三个阈值来让后台回收线程尽早工作，但是在 linux 中，我们只能调整 `pages_min` 页最小阈值，因为其他两个通过这个计算得到的。

  **缺点：**增大了 min_free_kbytes 配置后，这会使得系统预留过多的空闲内存，从而在一定程度上降低了应用程序可使用的内存量，这在一定程度上浪费了内存。极端情况下设置 min_free_kbytes 接近实际物理内存大小时，留给应用程序的内存就会太少而可能会频繁地导致 OOM 的发生。



#### 如何保护一个进程不被 OOM 杀掉？

在系统空闲内存不足的情况，进程申请了一个很大的内存，如果直接内存回收都无法回收出足够大的空闲内存，那么就会触发 OOM 机制，内核就会根据算法选择一个进程杀掉。

Linux 到底是根据什么标准来选择被杀的进程呢？这就要提到一个在 Linux 内核里有一个 `oom_badness()` 函数，它会把系统中可以被杀掉的进程扫描一遍，并对每个进程打分，得分最高的进程就会被首先杀掉。

进程得分的结果受下面这两个方面影响：

- 第一，进程已经使用的物理内存页面数。
- 第二，每个进程的 OOM 校准值 `oom_score_adj`。它是可以通过 `/proc/[pid]/oom_score_adj` 来配置的。我们可以在设置 -1000 到 1000 之间的任意一个数值，调整进程被 OOM Kill 的几率。



每个进程的 oom_score_adj 默认值都为 0，所以最终得分跟进程自身消耗的内存有关，消耗的内存越大越容易被杀掉。我们可以通过调整 oom_score_adj 的数值，来改成进程的得分结果：

- 如果你不想某个进程被首先杀掉，那你可以调整该进程的 oom_score_adj，从而改变这个进程的得分结果，降低该进程被 OOM 杀死的概率。
- 如果你想某个进程无论如何都不能被杀掉，那你可以将 oom_score_adj 配置为 -1000。

我们最好将一些很重要的系统服务的 oom_score_adj 配置为 -1000，比如 sshd，因为这些系统服务一旦被杀掉，我们就很难再登陆进系统了。

但是，不建议将我们自己的业务程序的 oom_score_adj 设置为 -1000，因为业务程序一旦发生了内存泄漏，而它又不能被杀掉，这就会导致随着它的内存开销变大，OOM killer 不停地被唤醒，从而把其他进程一个个给杀掉。



#### 32位系统能访问 4GB 以上的内存吗？

正常情况下是不可以的，因为32位系统的地址总线，最大只能表示$$2^{32}$$的地址，也就是4GB，而且在32位系统中，还有部分地址是无法访问的，所以内存只能识别3GB多。但是，目前有一种 PAE 技术，也就是物理地址扩展技术，将地址总线扩展为36位，最大的实体内存由 4GB 增加到 64GB 。



#### 在 4GB 物理内存的机器上，可以申请 8G 内存吗？

- 在 32 位操作系统，因为进程最大只能申请 3 GB 大小的虚拟内存，所以直接申请 8G 内存，会申请失败。
- 在 64位 位操作系统，因为进程最大只能申请 128 TB 大小的虚拟内存，即使物理内存只有 4GB，申请 8G 内存也是没问题，因为申请的内存是虚拟内存。如果这块虚拟内存被访问了，要看系统有没有 Swap 分区：
  - 如果没有 Swap 分区，因为物理空间不够，进程会被操作系统杀掉，原因是 OOM（内存溢出）；
  - 如果有 Swap 分区，即使物理内存只有 4GB，程序也能正常使用 8GB 的内存，进程可以正常运行；



需要考虑的前置条件：

- 操作系统是 32 位的，还是 64 位的？
- 申请完 8G 内存后会不会被使用？
- 操作系统有没有使用 Swap 机制？

> 操作系统是 32 位的，还是 64 位的？

因为 32 位操作系统，进程最多只能申请 3 GB 大小的虚拟内存空间，所以进程申请 8GB 内存的话，在申请虚拟内存阶段就会失败（失败的原因应该是 OOM）。64 位操作系统，进程可以使用 128 TB 大小的虚拟内存空间，所以进程申请 8GB 内存是没问题的，因为进程申请内存是申请虚拟内存，只要不读写这个虚拟内存，操作系统就不会分配物理内存。



> 如果申请完 8G 内存后，对这 8G 内存进行访问了

如果申请物理内存大小超过了空闲物理内存大小，就要看操作系统有没有开启 Swap 机制：

- 如果没有开启 Swap 机制，程序就会直接 OOM；
- 如果有开启 Swap 机制，程序可以正常运行。

**swap机制**：当系统的物理内存不够用的时候，就需要将物理内存中的一部分空间释放出来，以供当前运行的程序使用。那些被释放的空间可能来自一些很长时间没有什么操作的程序，这些被释放的空间会被临时保存到磁盘，等到那些程序要运行时，再从磁盘中恢复保存的数据到内存中。



> Swap 换入换出的是什么类型的内存？

内核缓存的文件数据，因为都有对应的磁盘文件，所以在回收文件数据的时候， 直接写回到对应的文件就可以了。

但是像进程的堆、栈数据等，它们是没有实际载体，这部分内存被称为匿名页。而且这部分内存很可能还要再次被访问，所以不能直接释放内存，于是就需要有一个能保存匿名页的磁盘载体，这个载体就是 Swap 分区。

匿名页回收的方式是通过 Linux 的 Swap 机制，Swap 会把不常访问的内存先写到磁盘中，然后释放这些内存，给其他更需要的进程使用。再次访问这些内存时，重新从磁盘读入内存就可以了。









#### cache 的级别？内存，L1，L2的具体耗时

<img src="./image/操作系统/cache层次结构.jpg"  />

L1—寄存器；L2，L3—高级缓存；L4—主存；L5—本地磁盘；L6—远程服务器上的存储

- L1：5个 CPU 时钟周期
- L2：10个 CPU 时钟周期
- L3： 40个 CPU 时钟周期
- L4（内存）：100个 CPU 时钟周期

主要思想：是上一层的存储器作为低一层存储器的高速缓存。



#### 内存覆盖和交换

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210627194447840.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3UwMTQxMTc5NDM=,size_16,color_FFFFFF,t_70)

1. **内存覆盖**：将程序分为多个模块，把内存空间分为一个“固定区”和若干个“覆盖区”。需要常驻内存的模块放在“固定区”中常驻，不常用的模块放在“覆盖区”，需要用到时换入内存，用不到时就换出内存。
2. **内存交换**：内存空间紧张时，系统将内存中某些进程暂时换出到外存中，把外存中某些已具备运行条件的进程换入到内存中。



### 堆栈相关

#### 堆和栈的区别？

1. **管理方式不同**。栈由操作系统自动分配释放，无需我们手动控制；堆的申请和释放工作由程序员控制，容易产生内存泄漏；
2. **空间大小不同**。每个进程拥有的栈的大小要远远小于堆的大小。理论上，程序员可申请的堆大小为虚拟内存的大小，进程栈的大小 64bits 的 Windows 默认 1MB，64bits 的 Linux 默认 10MB；
3. **生长方向不同**。堆的生长方向向上，内存地址由低到高；栈的生长方向向下，内存地址由高到低。
4. **分配方式不同**。堆都是动态分配的，没有静态分配的堆。栈有2种分配方式：静态分配和动态分配。静态分配是由操作系统完成的，比如局部变量的分配。动态分配由alloca函数进行分配，但是栈的动态分配和堆是不同的，他的动态分配是由操作系统进行释放，无需我们手工实现。
5. **分配效率不同**。栈由操作系统自动分配，会在硬件层级对栈提供支持：分配专门的寄存器存放栈的地址，压栈出栈都有专门的指令执行，这就决定了栈的效率比较高。堆则是由C/C++提供的库函数或运算符来完成申请与管理，实现机制较为复杂，频繁的内存申请容易产生内存碎片。显然，堆的效率比栈要低得多。
6. **存放内容不同**。
   - 栈存放的内容，函数返回地址、相关参数、局部变量和寄存器内容等。
   - 堆，一般情况堆顶使用一个字节的空间来存放堆的大小，而堆中具体存放内容是由程序员来填充的。





## 系统调用相关



### 系统调用的底层实现



#### epoll 是同步还是异步的？（腾讯一面）

**从 I/O 层面来看的话， epoll 是同步的**，因为 `epoll()` 本身就需要阻塞等待操作系统返回值。

**从消息处理层面来看， epoll 是异步的**，因为每个调用 `epoll()` 的事件，不需要阻塞等待 epoll 的返回值，只需要 epoll 返回后，通知事件即可。



#### 介绍一下 I/O 多路复用（网易）

1. I/O 多路复用技术是一种使用一个进程来同时监听多个 I/O 流状态的技术，一旦检测到某个文件描述符发生了我们关心的事件，操作系统就会通知程序进行相应的处理。Linux 下实现 I/O 复用的系统调用主要有 select、poll 和 epoll 。

2. **select**，将已连接的 Socket 都放到一个**文件描述符集合** fd_set（可以理解为位图），然后调用 select 函数将文件描述符集合**拷贝**到内核里，让内核来检查是否有网络事件产生，检查的方式很粗暴，就是通过**遍历**文件描述符集合的方式，当检查到有事件产生后，将此 Socket 标记为可读或可写， 接着再把整个文件描述符集合**拷贝**回用户态里，然后用户态还需要再通过**遍历**的方法找到可读或可写的 Socket，然后再对其处理。对于 select 这种方式，需要进行 **2 次「遍历」文件描述符集合**，一次是在内核态里，一个次是在用户态里 ，而且还会发生 **2 次「拷贝」文件描述符集合**，先从用户空间传入内核空间，由内核修改后，再传出到用户空间中。

3. **poll**，poll 不再用 BitsMap 来存储所关注的文件描述符，取而代之用动态数组，以链表形式来组织，突破了 select 的文件描述符个数限制，当然还会受到系统文件描述符限制。

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%A4%9A%E8%B7%AF%E5%A4%8D%E7%94%A8/epoll.png" alt="img" style="zoom: 50%;" />

4. **epoll**，epoll 在内核里使用**红黑树来跟踪进程所有待检测的文件描述字**，把需要监控的 socket 通过 `epoll_ctl()` 函数加入内核中的红黑树里，红黑树是个高效的数据结构，增删查一般时间复杂度是 `O(logn)` ，通过对这棵黑红树进行操作，这样就不需要像 select/poll 每次操作时都传入整个 socket 集合，只需要传入一个待检测的 socket，减少了内核和用户空间大量的数据拷贝和内存分配。epoll 使用事件驱动的机制，内核里**维护了一个链表来记录就绪事件**，当某个 socket 有事件发生时，通过回调函数内核会将其加入到这个就绪事件列表中，当用户调用 `epoll_wait()` 函数时，只会返回有事件发生的文件描述符的个数，不需要像 select/poll 那样轮询扫描整个 socket 集合，大大提高了检测的效率。



#### 介绍一下 epoll 的事件触发模式（字节）

epoll 支持两种事件触发模式，分别是**边缘触发（*edge-triggered，ET*）**和**水平触发（*level-triggered，LT*）**。

1. **ET模式**

   使用边缘触发模式，I/O 事件发生时只会通知一次，而且我们不知道到底能读写多少数据，所以在收到通知后应尽可能地读写数据，以免错失读写的机会。**边缘触发模式一般和非阻塞 I/O 搭配使用**，程序会一直执行 I/O 操作，直到系统调用（如 `read` 和 `write`）返回错误，错误类型为 `EAGAIN` 或 `EWOULDBLOCK`。

2. **LT模式**

   使用水平触发模式时，当被监控的 Socket 上有可读事件发生时，**服务器端不断地从 epoll_wait 中苏醒，直到内核缓冲区数据被 read 函数读完才结束**，目的是告诉我们有数据需要读取；

一般来说，边缘触发的效率比水平触发的效率要高，因为**边缘触发可以减少 epoll_wait 的系统调用次数**，系统调用也是有一定的开销的的，毕竟也存在上下文的切换。

select/poll 只有水平触发模式，epoll 默认的触发模式是水平触发，但是可以根据应用场景设置为边缘触发模式。

还有一个问题，在Linux下，select() 可能会将一个 socket 文件描述符报告为 "准备读取"，而后续的读取块却没有。此时如果使用非阻塞 I/O 则会造成程序阻塞在读的地方。（考虑这种情况：当数据已经到达，但经检查后发现有错误的校验和而被丢弃时，就会发生这种情况。也有可能在其他情况下，文件描述符被错误地报告为就绪。）



#### 五大 IO 模型是哪些？

阻塞 IO、非阻塞 IO、多路复用 IO、信号驱动式 IO、异步 IO

**阻塞 IO 模型**

<img src="https://mmbiz.qpic.cn/mmbiz_gif/GLeh42uInXTyY80RSpUTLjIMiaGGicv9zADM8nrhNkEtFpSpLjGicOemZ5mt7orYF8vFC7g83lPVDeSbnlgKl7XaA/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片" style="zoom:67%;" />

如果一个文件描述符设置为阻塞的，当应用调用 `read()` 的时候，应用会一直停在 `read()` 函数中，直到应用接收到数据，然后这个过程中会有两个阶段的阻塞，一是数据从网卡拷贝到内核缓存区中，二是从内核缓存区拷贝到用户缓存区。产生的结果就是如果对方一直不发送数据，那么应用会一直阻塞在这个地方。



**非阻塞 IO**

<img src="https://mmbiz.qpic.cn/mmbiz_gif/GLeh42uInXTyY80RSpUTLjIMiaGGicv9zAT6rHhibbzK5rXiarLuJU0P4MGrHNl35vVCV4JdS4FeejOkl8bBGz9nVQ/640?wx_fmt=gif&wxfrom=5&wx_lazy=1" alt="图片" style="zoom:67%;" />

非阻塞 IO 就是，当一个文件描述符设置为非阻塞，应用调用 `read()` 的时候，如果文件描述符读没有就绪的话，函数会直接返回 -1，避免用户阻塞在 `read()` 函数，当描述符读就绪后，才会将数据从内核缓冲区搬到用户缓冲区，这部分是阻塞的。非阻塞 IO 一般与 IO 多路复用配合使用。



**多路复用 IO**

I/O 多路复用技术是一种使用一个进程来同时监听多个 I/O 流状态的技术，一旦检测到某个文件描述符发生了我们关心的事件，操作系统就会通知程序进行相应的处理。Linux 下实现 I/O 复用的系统调用主要有 select、poll 和 epoll 。



**信号驱动 IO**

在信号驱动IO模型中，当用户线程发起一个IO请求操作，会给对应的socket注册一个信号函数，然后用户线程会继续执行，当内核数据就绪时会发送一个信号给用户线程，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。这个一般用于UDP中，对TCP套接口几乎是没用的，原因是该信号产生得过于频繁，并且该信号的出现并没有告诉我们发生了什么事情，而 UDP 的信号只有两个数据到达套接字和套接字发生错误。



**异步 IO**

前面四个实际上都是同步 IO，因为都需要应用参与到读取数据的过程中，要么需要等待数据从网卡拷贝到内核缓冲区，要么需要等待数据从内核缓冲区拷贝到用户缓冲区。而异步IO的优化思路是解决了应用程序需要先后发送询问请求、发送接收数据请求两个阶段的模式，在异步IO的模式下，只需要向内核发送一次请求就可以完成状态询问和数拷贝的所有操作。



#### sleep、wait 、join、yield的区别

##### sleep

- sleep是一个延时函数，让进程或线程进入休眠。休眠完毕后继续运行。
- 在linux下面，sleep函数的参数是秒，而windows下面sleep的函数参数是毫秒。
- 头文件 `#include <unistd.h>`



##### wait

- wait是父进程回收子进程PCB资源的一个系统调用。
- 进程一旦调用了wait函数，就立即阻塞自己本身，然后由wait函数自动分析当前进程的某个子进程是否已经退出，当找到一个已经变成僵尸的子进程，wait就会收集这个子进程的信息，并把它彻底销毁后返回；如果没有找到这样一个子进程，wait就会一直阻塞，直到有一个出现为止。



##### join

是一个等待线程函数，主线程需等待子线程运行结束后才可以结束（注意不是才可以运行，运行是并行的），如果打算等待对应线程，则需要细心挑选调用 join() 的位置



##### 区别

1. sleep() 使线程进入阻塞状态（线程睡眠），wait() 使线程进入等待队列（线程挂起），阻塞类型不同
1. 



### epoll系列函数

1. `int epoll_create(int size)`

   - size 只要大于0即可

2. `int epoll_ctl(int epfd, int op, int fd, struct epoll_event* event)`

   - epfd：epoll_create的返回值

   - op：对红黑树的操作，增加、删除、修改节点

     添加事件：相当于往红黑树添加一个节点，每个客户端连接服务器后会有一个通讯套接字，每个连接的通讯套接字都不重复，所以这个通讯套接字就是红黑树的 key。

     修改事件：把红黑树上监听的 socket 对应的监听事件做修改。

     删除事件：相当于取消监听 socket 的事件。

   - fd：需要添加监听的fd

   - event：设置感兴趣的事件信息

3. `int epoll_wait(int epfd, struct epoll_event* event, int maxevents, int timeout)`

   - epid：epoll_create的返回值
   - events：存放就绪的事件集合，是传出参数
   - maxevents：可以存放的事件个数
   - 阻塞等待的事件长短，-1代表阻塞等待



### socket系列函数

1. `int socket(int domain, int type, int protocol)`

   - domain：协议域，AF_INET/AF_INET6/AF_LOCAL，决定使用的socket地址类型
   - type：socket类型，SOCK_STREAM、SOCK_DGRAM
   - protocol：协议，IPPROTO_TCP、IPPTOTO_UDP

2. `int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`

   - sockfd：通过socket()函数创建的fd，bind()函数就是将给这个fd绑定一个名字。

   - addr：一个const struct sockaddr *指针，指向要绑定给sockfd的协议地址。

     ```c
     struct sockaddr_in {
         sa_family_t    sin_family; /* address family: AF_INET */
         in_port_t      sin_port;   /* port in network byte order */
         struct in_addr sin_addr;   /* internet address */
     };
     
     /* Internet address. */
     struct in_addr {
         uint32_t       s_addr;     /* address in network byte order */
     };
     ```

   - addrlen：对应的是地址的长度。

3. `int listen(int sockfd, int backlog)`

   - sockfd：要监听的socket描述字
   - backlog：可以排队的最大连接个数

4. `int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen)`

   - sockfd：要监听的socket描述字
   - addr：服务器的socket地址
   - addrlen：socket的长度

5. `int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen)`

   - sockfd：服务器的socket描述字
   - addr：返回客户端的协议地址
   - addrlen：协议地址的长度



### 文件系统

#### open和fopen的区别是什么？

1. **来源不同**：`open` 是 UNIX 的系统调用函数，返回文件描述符。而 `fopen` 是C语言的库函数，在不同的系统中会调用不同的系统api。返回一个指向文件结构的指针。
2. **移植性**：`fopen` 是C标准函数，因此拥有良好的移植性；而 `open` 是UNIX系统调用，移植性有限。
3. **适用范围**：因为在Linux下，一切设备都是文件，所以Linux下的接口 `open` 适用于硬件、普通文件、网络套接字等。而 `fopen` 只适用于普通文件。
4. **有无缓冲**：`open` 无缓冲，`fopen` 有缓冲。前者与 `read`, `write` 等配合使用， 后者与`fread` , `fwrite` 等配合使用。





## 进程线程协程相关



### 进程

#### PCB的结构

<img src="./image/操作系统/pcb.jpg" style="zoom: 50%;" />

**1、进程描述信息：**

- 进程标识符：标识各个进程，每个进程都有一个并且唯一的标识符；
- 用户标识符：进程归属的用户，用户标识符主要为共享和保护服务；

**2、进程控制和管理信息**

- 进程当前状态，如 new、ready、running、waiting 或 blocked 等；
- 进程优先级：进程抢占 CPU 时的优先级；

**3、资源分配清单**

- 有关内存地址空间或虚拟地址空间的信息，所打开文件的列表和所使用的 I/O 设备信息。

**4、CPU相关信息**

- CPU 中各个寄存器的值，当进程被切换时，CPU 的状态信息都会被保存在相应的 PCB 中，以便进程重新执行时，能从断点处继续执行。



#### 进程和线程的区别？

1. **根本区别**：进程是操作系统进行资源分配的最小单元，线程是操作系统进行运算调度的最小单元。
2. **从属关系**：进程中包含了线程，线程属于进程。
2. **数据共享和同步**：进程的数据共享复杂，需要使用进程间通信，同步较为简单。因为同一进程的所有线程共享数据，但是同步复杂。
3. **拥有的资源不同**：进程中拥有独立的堆栈、寄存器、程序计数器、虚拟地址空间（也就是页表）、打开的文件描述符等。线程中只拥有自己独立的程序计数器、堆栈和寄存器。因此，进程的切换开销要比线程要大。
4. **控制和影响能力不同**：子线程如果崩溃了，会导致父线程也崩溃，但是子进程崩溃不会影响父进程。



#### 进程组和会话

1. 进程组是一组相关进程的集合，会话是一组相关进程组的集合。
2. 一个进程持有 ID
   - PID：进程的唯一标识。对于多线程的进程而言，所有线程调用getpid函数会返回相同的值。
   - PGID：进程组ID。每个进程都会有进程组ID，表示该进程所属的进程组。默认情况下新创建的进程会继承父进程的进程组ID。
   - SID：会话ID。每个进程也都有会话ID。默认情况下，新创建的进程会继承父进程的会话ID。
3. s



#### 互斥与同步的区别？

- **同步**，是指多个线程（或进程）为了合作完成任务，必须严格按照规定的某种先后次序来运行。
- **互斥**，是指系统中的某些共享资源，一次只允许一个线程访问。当一个线程正在访问该临界资源时，其它线程必须等待。但互斥无法限制访问者对资源的访问顺序，即访问是无序的。



#### 请你说说什么是守护进程，如何实现？

1. 定义：守护进程是运行在后台的一种生存期长的特殊进程。它独立于控制终端，处理一些系统级任务。

2. 实现：

   <img src="https://img-blog.csdn.net/20161209165658384" alt="img" style="zoom: 80%;" />

   （1）创建子进程，终止父进程。调用 `fork()` 产生一个子进程，然后使父进程退出。

   （2）调用 `setsid()` 创建一个新会话

   - 使当前进程脱离原会话的控制
   - 使当前进程脱离原进程组的控制
   - 使当前进程脱离原控制终端的控制

   （3）将当前目录更改为根目录。使用 `fork()` 创建的子进程也继承了父进程的当前工作目录

   （4）重设文件权限掩码，文件权限掩码是指屏蔽掉文件权限中的对应位。

   （5）关闭不再需要的文件描述符。子进程从父进程继承打开的文件描述符。



#### 说说进程通信的方式有哪些？

- [ ] 七种

进程间通信主要包括**管道**（有名管道和无名管道）、**系统IPC**（包括消息队列、信号量、信号、共享内存）、**套接字socket**、`eventfd`。

1. **管道**：包括无名管道和有名管道。
   
   - **无名管道** `int pipe(int fd[2]);`
     - 无名管道是一种特殊的文件，这种文件只存在于内存中。
     - 无名管道只能用于父子进程或兄弟进程之间，必须用于具有亲缘关系的进程间的通信。
   
     - 无名管道只能由一端向另一端发送数据，是半双工方式，如果双方需要同时收发数据需要两个管道。
   - **有名管道** `int mkfifo(const char *pathname, mode_t mode);`
     - 有名管道是FIFO文件，存在于文件系统中，可以通过文件路径名来指出。
     - 无名管道可以在不具有亲缘关系的进程间进行通信。
   
2. **系统IPC**
   
   - **消息队列**：消息的链表，放在内核中。消息队列独立于发送与接收进程，进程终止时，消息队列及其内容并不会被删除；消息队列可以实现消息的随机查询，可以按照消息的类型读取。
     
     - 存在用户态与内核态之间的数据拷贝开销
     - 附件也有大小限制
     
   - **信号量semaphore**：信号量是一个计数器，用于多线程对共享数据的访问，信号量的意图在于进程间同步。这种通信方式主要用于解决与同步相关的问题并避免竞争条件。
   
     - 通常**信号量表示资源的数量**，对应的变量是一个整型（`sem`）变量。
   
       另外，还有**两个原子操作的系统调用函数来控制信号量的**，分别是：
   
       - *P 操作*：将 `sem` 减 `1`，相减后，如果 `sem < 0`，则进程/线程进入阻塞等待，否则继续，表明 P 操作可能会阻塞；
       - *V 操作*：将 `sem` 加 `1`，相加后，如果 `sem <= 0`，唤醒一个等待中的进程/线程，表明 V 操作不会阻塞；
   
   - **信号**：用于通知接收进程某个事件的发生。
     
     - 信号是进程间通信机制中**唯一的异步通信机制**
     
   - **共享内存**：使得多个进程可以访问同一块内存空间，不同进程可以及时看到对方进程内存中数据的更新。这种方式需要依靠某种同步操作，如互斥锁和信号量等。可以说这是最有用的进程间通信方式。
     - 多进程并发下，最终结果是不可预期的。所以对这块临界区的访问需要通过**信号量**来进行进程同步。
   
3. **套接字socket**：用于不同主机直接的通信。





#### 进程间同步的方式？

1. **信号量semaphore**：是一个计数器，可以用来控制多个进程对共享资源的访问。信号量用于实现进程间的互斥与同步。P操作(递减操作)可以用于阻塞一个进程，V操作(增加操作)可以用于解除阻塞一个进程。
2. **管道**：一个进程通过调用管程的一个过程进入管程。在任何时候，只能有一个进程在管程中执行，调用管程的任何其他进程都被阻塞，以等待管程可用。
3. **消息队列**：消息的链接表，放在内核中。消息队列独立于发送与接收进程，进程终止时，消息队列及其内容并不会被删除；消息队列可以实现消息的随机查询，可以按照消息的类型读取。（生产消费者模型）



#### 说说线程间通信的方式有哪些？

线程间的通信方式包括**临界区、互斥量、信号量、条件变量、读写锁**：

1. 临界区：每个线程中访问临界资源的那段代码称为临界区（Critical Section）（临界资源是一次仅允许一个线程使用的共享资源）。每次只准许一个线程进入临界区，进入后不允许其他线程进入。不论是硬件临界资源，还是软件临界资源，多个线程必须互斥地对它进行访问。
2. 互斥量：采用互斥对象机制，只有拥有互斥对象的线程才可以访问。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。
3. 信号量：计数器，允许多个线程同时访问同一个资源。
4. 条件变量：通过条件变量通知操作的方式来保持多线程同步。
5. 读写锁：读写锁与互斥量类似。但互斥量要么是锁住状态，要么就是不加锁状态。读写锁一次只允许一个线程写，但允许一次多个线程读，这样效率就比互斥锁要高。



#### 说说线程同步方式有哪些？

线程间的同步方式包括**互斥锁、信号量、条件变量、读写锁**：

1. **互斥锁**：采用互斥对象机制，只有拥有互斥对象的线程才可以访问。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问。
2. **信号量**：计数器，允许多个线程同时访问同一个资源。
3. **条件变量**：通过条件变量通知操作的方式来保持多线程同步。
4. **读写锁**：读写锁与互斥量类似。但互斥量要么是锁住状态，要么就是不加锁状态。读写锁一次只允许一个线程写，但允许一次多个线程读，这样效率就比互斥锁要高。



#### 说一下进程的上下文切换和线程的上下文切换

**（1）定义**：上下文切换的指的是内核在 CPU 上对线程或者线程进行切换

**（2）进程上下文切换：**

1. 保护被中断进程的 CPU 硬件上下文（PC和CPU 寄存器）到 PCB
2. 修改被中断进程的 PCB 有关信息，如进程状态等
3. 将被中断进程的 PCB 加入相关队列（如何是因为阻塞被切换，就加入阻塞队列；如果因为时间片用完被切换，就加入就绪队列）
4. 内核进行进程调度，从就绪队列中选择下一个运行的进程
5. 根据被选中的进程设置操作系统用到的地址转换和存储保护信息
   - 切换页表，以使用新的地址空间
   - 切换内核栈和硬件上下文（包括分配的内存、数据段、堆栈段等）
6. 根据被选中进程的 PCB 恢复处理器现场

**（3）线程上下文切换：**

**线程的上下文切换需要看两个线程是否属于同一进程**，如果不属于则与进程的上下文切换相同。

如果属于则：

1. 保护被中断进程的 CPU 硬件上下文（PC和CPU 寄存器）到 TCB
2. 修改被中断进程的 TCB 有关信息，如进程状态等
3. 将被中断进程的 TCB 加入相关队列（如何是因为阻塞被切换，就加入阻塞队列；如果因为时间片用完被切换，就加入就绪队列）
4. 内核进行进程调度，从就绪队列中选择下一个运行的进程
5. 根据被选中的进程设置操作系统用到的地址转换和存储保护信息
   - 切换内核栈和硬件上下文（包括分配的内存、数据段、堆栈段等）
6. 根据被选中进程的 TCB 恢复处理器现场



#### linux一个进程要删除另一个进程正在写入的文件，能不能删除成功？

对于删除文件的进程来说是删除成功了，而对于写入文件的进程来说，文件依然存在。

因为，Linux 是通过 link 的数量来控制文件删除，只有当一个文件不存在任何 link 的时候，这个文件才会被删除。每个文件都有 2 个 link 计数器  i_count 和 i_nlink 。i_count 的意义是当前使用者的数量，i_nlink 的意义是介质连接的数量；

此时进程调用 rm 删除文件，会使得 i_nlink 减小，但由于 i_count 不为0，所以文件不会立即被删除，等到进程关闭文件后，文件才会由操作系统进行删除。

这里有一种特殊的情况，就是如果对于动态库的更新，需要先 rm 在 cp，不能直接 cp，因为存在内存映射区与磁盘文件的自动同步，会导致程序崩溃。



#### 孤儿进程和僵尸进程有什么区别？如何解决僵尸进程？

##### 孤儿进程和僵尸进程有什么区别？

1. **僵尸进程**：一个进程调用fork函数创建了一个子进程，但是子进程先执行结束，而父进程没有调用 `wait` 和 `waitpid` 获取子进程的状态，那么子进程的状态信息会一直保存在系统中，这时的子进程就叫做僵尸进程。
2. **孤儿进程**：一个进程调用了fork函数创建了一个子进程，但是子进程还在执行的时候，父进程由于某种原因先退出了，那么这个父进程的所有子进程都会变成孤儿进程。

> 僵尸进程的危害

导致僵尸进程的进程号一直被占用，而系统所能使用的进程号是有限的，如果系统存在大量的僵尸进程，系统会因为没有可用的进程号而导致系统不能产生新的进程。



##### 如何解决僵尸进程？

1. **直接 kill 父进程**：将僵尸进程变为孤儿进程，由 init 进程进行接管（PID = 1），完成子进程的回收工作。
2. **通过信号机制**：子进程在退出时会向父进程发送 SIGCHILD 信号。父进程可以通过 `signal(SIGCHLD, sig_child);` 设置信号处理函数，在信号处理函数中调用 wait 处理僵尸进程。
3. **fork()两次**：父进程创建一个子进程，子进程再创建一个孙进程，然后子进程退出，孙进程由 init 进程接管（PID = 1），当然子进程还是需要父进程进行回收。





## 网络系统相关

#### 服务器单机理论的最大并发数是多少？

这个问题需要分客户端和服务端考虑。记住一个前提：一个 TCP 连接是由一个四元组决定的（源 IP、源端口、目标 IP 、目标端口）。

1. **对于客户端**

   目标 IP 和目标端口是确定的，而对于一个网卡的主机来说，源 IP 也是确定的，所以客户端的连接数据是由端口数目决定的。端口号是 16 位的，所以端口数为 $$2^{16} = 65536$$，但是 Linux 对可使用的端口范围有限制，具体可以通过以下命令查看：

   ```bash
   cat /proc/sys/net/ipv4/ip_local_port_range 
   1024 65000
   ```

2. **对于服务端**

   目标端口和目标 IP 是确定的，所以对于服务端来说，最大的连接数就是源 IP 数乘以源端口数，也就是大约 $$2^{32} \cdot 2^{16} = 2^{48}$$（IPv4 为32位，端口号为16位）。但实际上服务器肯定承载不了这么大的连接数，会回到两方面的限制：

   - **文件描述符**， Socket 实际上是一个文件，也就会对应一个文件描述符。在 Linux 下，单个进程打开的文件描述符数是有限制的，没有经过修改的值一般都是 1024，不过我们可以通过 ulimit 增大文件描述符的数目；
   - **系统内存**，每个 TCP 连接在内核中都有对应的数据结构，意味着每个连接都是会占用一定内存的；



#### Reactor 模式和 Proactor 模式听过吗？有什么区别？（字节二面）

##### Reactor 模式

Reactor 模式也叫 `Dispatcher` 模式，即 **I/O 多路复用监听事件，收到事件后，根据事件类型分配（Dispatch）给某个进程 / 线程**。Netty、Nginx 和 MemCache 用的都是这种方案。

Reactor 模式主要由 Reactor 和处理资源池这两个核心部分组成，它俩负责的事情如下：

- Reactor 负责监听和分发事件，事件类型包含连接事件、读写事件；
- 处理资源池负责处理事件，如 read -> 业务逻辑 -> send；

Reactor 模式是灵活多变的，可以应对不同的业务场景，灵活在于：

- Reactor 的数量可以只有一个，也可以有多个；
- 处理资源池可以是单个进程 / 线程，也可以是多个进程 /线程；

主要有三种形式：单 Reactor 单线程 -> 单 Reactor 多线程 -> 多 Reactor 多线程

1. **单 Reactor 单线程**

   Redis 用的就是这种模式。

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png" alt="img" style="zoom: 50%;" />

   - Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发（根据事件类型）；
   - 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
   - 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
   - Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

   缺点：因为只有一个进程，**无法充分利用 多核 CPU 的性能**；Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，**如果业务处理耗时比较长，那么就造成响应的延迟**；

2. **单 Reactor 多线程**

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

   - Handler 对象不再负责业务处理，只负责数据的接收和发送，Handler 对象通过 read 读取到数据后，会将数据发给子线程里的 Processor 对象进行业务处理；
   - 子线程里的 Processor 对象就进行业务处理，处理完后，将结果发给主线程中的 Handler 对象，接着由 Handler 通过 send 方法将响应结果发送给 client；

   单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

   「单 Reactor」的模式还有个问题，**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方**。

   

3. **多 Reactor 多线程**

   Netty、Nginx 和 MemCache都用的这个框架

   <img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E4%B8%BB%E4%BB%8EReactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png" alt="img" style="zoom:50%;" />

   - 主线程中的 MainReactor 对象通过 select 监控连接建立事件，收到事件后通过 Acceptor 对象中的 accept 获取连接，将新的连接分配给某个子线程；
   - 子线程中的 SubReactor 对象将 MainReactor 对象分配的连接加入 select 继续进行监听，并创建一个 Handler 用于处理连接的响应事件。
   - 如果有新的事件发生时，SubReactor 对象会调用当前连接对应的 Handler 对象来进行响应。
   - Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。



##### Proactor

Reactor 是非阻塞同步网络模式，而 **Proactor 是异步网络模式**。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/Proactor.png" alt="img" style="zoom:50%;" />

- Proactor Initiator 负责创建 Proactor 和 Handler 对象，并将 Proactor 和 Handler 都通过 Asynchronous Operation Processor 注册到内核；
- Asynchronous Operation Processor 负责处理注册请求，并处理 I/O 操作；
- Asynchronous Operation Processor 完成 I/O 操作后通知 Proactor；
- Proactor 根据不同的事件类型回调不同的 Handler 进行业务处理；
- Handler 完成业务处理；

在 Linux 下的异步 I/O 是不完善的， `aio` 系列函数是由 POSIX 定义的异步操作接口，不是真正的操作系统级别支持的，而是在用户空间模拟出来的异步，并且仅仅支持基于本地文件的 aio 异步操作，**网络编程中的 socket 是不支持的**，这也使得基于 Linux 的高性能网络程序都是使用 Reactor 方案。



##### 区别

- **Reactor 是非阻塞同步网络模式，感知的是就绪可读写事件**。在每次感知到有事件发生（比如可读就绪事件）后，就需要应用进程主动调用 read 方法来完成数据的读取，也就是要应用进程主动将 socket 接收缓存中的数据读到应用进程内存中，这个过程是同步的，读取完数据后应用进程才能处理数据。
- **Proactor 是异步网络模式， 感知的是已完成的读写事件**。在发起异步读写请求时，需要传入数据缓冲区的地址（用来存放结果数据）等信息，这样系统内核才可以自动帮我们把数据的读写工作完成，这里的读写工作全程由操作系统来做，并不需要像 Reactor 那样还需要应用进程主动发起 read/write 来读写数据，操作系统完成读写工作后，就会通知应用进程直接处理数据。



#### 负载均衡算法

**轮询**，只能适用与每个节点的数据都是相同的场景，访问任意节点都能请求到数据。但是**不适用分布式系统**，因为分布式系统意味着数据水平切分到了不同的节点上，访问数据的时候，一定要寻址存储该数据的节点。

**哈希算法**，也就是 KV 存储，虽然能建立数据和节点的映射关系，但是每次在节点数量发生变化的时候，最坏情况下所有数据都需要迁移，这样太麻烦了，所以不适用节点数量变化的场景。

**一致性哈希算法**，为了减少迁移的数据量，将「存储节点」和「数据」都映射到一个首尾相连的**哈希环上**，如果增加或者移除一个节点，仅影响该节点在哈希环上顺时针相邻的后继节点，其它数据也不会受到影响。但是一致性哈希算法不能够均匀的分布节点，会出现大量请求都集中在一个节点的情况，在这种情况下进行容灾与扩容时，容易出现雪崩的连锁反应。

**虚拟节点**，对一个真实节点做多个副本。不再将真实节点映射到哈希环上，而是将虚拟节点映射到哈希环上，并将虚拟节点映射到实际节点，所以这里有「两层」映射关系。可以会提高节点的均衡度，还会提高系统的稳定性。所以，带虚拟节点的一致性哈希方法不仅适合硬件配置不同的节点的场景，而且适合节点规模会发生变化的场景。



#### 在进行网络通信时是否需要进行字节序转换？

相同字节序的平台在进行网络通信时可以不进行字节序转换，但是跨平台进行网络数据通信时必须进行字节序转换。网络字节序是大端。

**UDP/TCP/IP协议规定：把接收到的第一个字节当作高位字节看待，存放到低地址；**





#### 网络通信的开销主要在哪里，如何优化？

##### 主要开销

网络通信的主要开销在协议栈。在传统的内核协议栈中，当有新的数据包到来时，网卡会通过硬件中断通知协议栈，内核的网卡驱动负责处理这个硬件的中断，将数据包从网卡拷贝到内核开辟的缓冲区中。**协议栈的主要处理开销分为中断处理、内存拷贝、系统调用和文件处理四个方面**。



##### 优化

1. **中断优化**

   - **在流量小时使用中断模式，当遇到大流量时切换为轮询模式**：中断处理方式在低速网络I/O场景下非常有效，高速网络中，随着网络 I/O速率的不断上升，网卡面对大量高速数据分组引发频繁的中断，中断引起的上下文切换开销将变得不可忽视，造成较高的时延，并引起吞吐量下降。

2. **内存复制开销优化**

   - **零拷贝技术**：将网卡数据通过 DMA 直接复制到内存（不需要 CPU 参与）

3. **上下文切换开销优化**

   - **用户态协议栈**：直接将协议栈移动到应用层实现，避免昂贵的上下文切换开销， 如mTCP、 lwIP等方案。
   - **批处理系统调用**：批处理系统调用平摊开销，MegaPipe、FlexSC、 VOS均使用批量系统调用来减小开销。

4. **文件系统开销**

   socket是一种文件抽象，Linux 为了实现统一文件管理，通过虚拟文件系统（virtual file system，**VFS**）为套接字绑定了一系列对应的数据结构如inode、dentry等。这些数据结构对于网络通信的用处不大，是额外的开销。

   - **自定义轻量级 Socket**：在**用户态重新定义实现socket**结构体。
   - **简化 VFS 的 socket 实现**：继承VFS的socket实现，但是简化掉inode与 dentry 的初始化与销毁过程，抛弃其中的锁。



#### 如果某些数据包被 iptables 封禁，是否可以通过 tcpdump 抓到包？

可以的，因为 tcpdump 是工作在协议栈之前，所以可以抓到主机收到的包。

网络包进入主机后的顺序如下：

- 进来的顺序  网卡 eth0-> 网络中断 -> ***tcpdump -> netfilter/iptables***
- 出去的顺序  ***netfilter/iptables -> tcpdump*** -> 网络中断 ->网卡 eth0



Iptables 是基于 Netfilter 框架实现的报文选择系统，其可以用于报文的过滤、网络地址转换和报文修改等功能。Netfilter 有五个节点：

- `NF_IP_PRE_ROUTING`: 位于路由之前，报文一致性检查之后（报文一致性检查包括: 报文版本、报文长度和checksum）。
- `NF_IP_LOCAL_IN`: 位于报文经过路由之后，并且目的是本机的。
- `NF_IP_FORWARD`：位于在报文路由之后，目的地非本机的。
- `NF_IP_LOCAL_OUT`: 由本机发出去的报文，并且在路由之前。
- `NF_IP_POST_ROUTING`: 所有即将离开本机的报文。

```text
--->[NF_IP_PRE_ROUTING]--->[ROUTE]--->[NF_IP_FORWARD]--->[NF_IP_POST_ROUTING]--->
                              |                        ^
                              |                        |
                              |                     [ROUTE]
                              v                        |
                       [NF_IP_LOCAL_IN]        [NF_IP_LOCAL_OUT]
                              |                        ^
                              |                        |
                              v                        |
```



### Linux 命令相关

#### 如何查看网络配置？

主要有两个命令 `ifconfig` 或者 `ip` ：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/showeth0.png" alt="img" style="zoom:50%;" />

第一，网口的连接状态标志。其实也就是表示对应的网口是否连接到交换机或路由器等设备，如果 `ifconfig` 输出中看到有 `RUNNING`，或者 `ip` 输出中有 `LOWER_UP`，则说明物理网络是连通的，如果看不到，则表示网口没有接网线。

第二，MTU 大小。默认值是 `1500` 字节，其作用主要是限制网络包的大小，如果 IP 层有一个数据报要传，而且网络包的长度比链路层的 MTU 还大，那么 IP 层就需要进行分片，即把数据报分成干片，这样每一片就都小于 MTU。事实上，每个网络的链路层 MTU 可能会不一样，所以你可能需要调大或者调小 MTU 的数值。

第三，网口的 IP 地址、子网掩码、MAC 地址、网关地址。这些信息必须要配置正确，网络功能才能正常工作。

第四，网络包收发的统计信息。通常有网络收发的字节数、包数、错误数以及丢包情况的信息，如果 `TX`（发送） 和 `RX`（接收） 部分中 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，则说明网络发送或者接收出问题了。



#### socket 信息如何查看？

可以使用 `netstat` 或者 `ss`，这两个命令查看 socket、网络协议栈、网口以及路由表的信息。但是 `ss` 的性能更好。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost3@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E7%BD%91%E7%BB%9C/showsocket.png" alt="img" style="zoom:50%;" />

包含 socket 的状态（*State*）、接收队列（*Recv-Q*）、发送队列（*Send-Q*）、本地地址（*Local Address*）、远端地址（*Foreign Address*）、进程 PID 和进程名称（*PID/Program name*）等信息。

接收队列（*Recv-Q*）和发送队列（*Send-Q*），在不同的 socket 状态。它们表示的含义是不同的。

当 socket 状态处于 `Established` 时：

- *Recv-Q* 表示 socket 缓冲区中还没有被应用程序读取的字节数；
- *Send-Q* 表示 socket 缓冲区中还没有被远端主机确认的字节数；

而当 socket 状态处于 `Listen` 时：

- *Recv-Q* 表示全连接队列的长度；
- *Send-Q* 表示全连接队列的最大长度；

**注意：**全连接队列指的是服务器与客户端完了 TCP 三次握手后，还没有被 `accept()` 系统调用取走连接的队列。



#### 网络吞吐量和 PPS 如何查看？

使用 `sar` 命令当前网络的吞吐率和 PPS，用法是给 `sar` 增加 `-n` 参数就可以查看网络的统计信息，比如

- sar -n DEV，显示网口的统计数据；
- sar -n EDEV，显示关于网络错误的统计数据；
- sar -n TCP，显示 TCP 的统计数据

对于带宽，我们可以使用 `ethtool` 命令来查询，它的单位通常是 `Gb/s` 或者 `Mb/s`，不过注意这里小写字母 `b` ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特（*bit*）



#### 连通性和延时如何查看？

要测试本机与远程主机的连通性和延时，通常是使用 `ping` 命令，它是基于 ICMP 协议的，工作在网络层。

显示的内容主要包含 `icmp_seq`（ICMP 序列号）、`TTL`（生存时间，或者跳数）以及 `time` （往返延时），而且最后会汇总本次测试的情况，如果网络没有丢包，`packet loss` 的百分比就是 0。

不过，需要注意的是，`ping` 不通服务器并不代表 HTTP 请求也不通，因为有的服务器的防火墙是会禁用 ICMP 协议的。



#### 如何从日志分析 PV、UV

**1、不要在线上环境 `cat` 日志文件**

当我们要分析日志的时候，先用 `ls -lh` 命令查看日志文件的大小，如果日志文件大小非常大，最好不要在线上环境做。（如果日志文件数据量太大，你直接一个 `cat` 命令一执行，是会影响线上环境，加重服务器的负载，严重的话，可能导致服务器无响应。）

当发现日志很大的时候，可以使用 `scp` 命令将文件传输到闲置的服务器再分析。

2、对于大文件先使用 `less` 、`tail` 、 `tail -f` （实时显示日志），读出部分页，没必要一次全部读出。



##### PV（page view）

PV 的全称叫 *Page View*，用户访问一个页面就是一次 PV，比如大多数博客平台，点击一次页面，阅读量就加 1，所以说 PV 的数量并不代表真实的用户数量，只是个点击量。

对于 nginx 的 `acess.log` 日志文件来说，分析 PV 还是比较容易的，既然日志里的内容是**访问记录**，那有多少条日志记录就有多少 PV。

所以统计行即可：`wc -l access.log` （`wc` ，word count，统计单词数和行数）

**统计每日的访问次数：**

1. 将日期分割出来
2. 按日期排序
3. 然后去重

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/log/awkpv.png" alt="img" style="zoom:50%;" />



##### UV（uniq visitor）

UV 的全称是 *Uniq Visitor*，它代表访问人数，比如公众号的阅读量就是以 UV 统计的，不管单个用户点击了多少次，最终只算 1 次阅读量。

access.log 日志里虽然没有用户的身份信息，但是我们可以用**「客户端 IP 地址」**来**近似统计** UV。

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/log/uv.png" alt="img" style="zoom:50%;" />



##### 终端分析

nginx 的 access.log 日志最末尾关于 User Agent 的信息，主要是客户端访问服务器使用的工具，可能是手机、浏览器等。

因此，我们可以利用这一信息来分析有哪些终端访问了服务器。

User Agent 的信息在日志里的第 12 列，因此我们先使用 `awk` 过滤出第 12 列的内容后，进行 `sort` 排序，再用 `uniq -c` 去重并统计，最后再使用 `sort -rn`（*r 表示逆向排序， n 表示按数值排序*） 对统计的结果排序，结果如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/log/terminal.png" alt="img" style="zoom:33%;" />



##### 分析 TOP3 请求

access.log 日志中，第 7 列是客户端请求的路径，先使用 `awk` 过滤出第 7 列的内容后，进行 `sort` 排序，再用 `uniq -c` 去重并统计，然后再使用 `sort -rn` 对统计的结果排序，最后使用 `head -n 3` 分析 TOP3 的请求，结果如下图：

<img src="https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E7%BD%91%E7%BB%9C/log/TOP3.png" alt="img" style="zoom:50%;" />



#### 什么是大端小端，如何判断？

**小端**：**低**的有效字节存储在**低的**存储器地址。小端一般为主机字节序；常用的X86结构是小端模式。很多的ARM，DSP都为小端模式。

**大端**：**高**的有效字节存储在**低的**存储器地址。大端为网络字节序；KEIL C51则为大端模式。

**判断方法：**

使用union，因为联合体变量从低地址存储。共用体的各个成员是共用一段内存的。1 是数据的低位，如果 1 被存储在 data 的低字节，就是小端模式，这个时候 data.ch 的值也是 1。如果 1 被存储在 data 的高字节，就是大端模式，这个时候 data.ch 的值就是 0。

<img src="https://img-blog.csdn.net/20170305103408849" alt="img" style="zoom: 67%;" />

```c++
#include <stdio.h>
int main() {
    union {
        int n;
        char ch;  // 可以看作 n 的低地址
    } data;
    data.n = 0x00000001;  //也可以直接写作 data.n = 1;
    if(data.ch == 1) {
        printf("Little-endian\n");
    }
    else {
        printf("Big-endian\n");
    }
    return 0;
}
```

> 为什么要区分大端与小端？

- cpu硬件设计中，为了方便计算一般都是从低位计算的，小端存储方式有利于低位计算。
- 但是小端方式不利于我们阅读，网络上进行抓包时使用大端方式解决了这个问题。





#### 服务器CPU 100%怎么定位？

先用top定位出问题的进程，再用top定位到出问题的线程，再打印线程堆栈查看运行情况。



#### 说一下常用的 linux 命令

ls/ll、cd、mkdir、rm -rf、cp、mv、kill、touch、tree

ps -ef | grep xxx、top

> 说一下 ps 的底层实现

ps 的作用是输出进程状态，然后在 linux 下，一个进程的所有信息都保存在 PCB（进程控制块）中，所以我觉得应该是在内核态遍历 PCB的链表 ，来获取所有进程的信息。





## 各种各样的锁

### 死锁

#### 死锁产生的条件

- [ ] 互斥 + 不可剥夺 + 请求和保持 + 循环等待

1. **互斥条件**：多个线程不能同时使用同一个资源。
2. **持有并等待条件**：线程在等待其他资源的同时并不会释放自己已经持有的资源。
3. **不可剥夺条件**：当线程已经持有了资源 ，**在自己使用完之前不能被其他线程获取**，线程 B 如果也想使用此资源，则只能在线程 A 使用完并释放后才能获取。
4. **环路等待条件**：在死锁发生的时候，**两个线程获取资源的顺序构成了环形链**。



#### 如何避免死锁的发生

- [ ] 加锁顺序 + 加锁时限

产生死锁的四个必要条件是：互斥条件、持有并等待条件、不可剥夺条件、环路等待条件。

那么避免死锁问题就只需要破环其中一个条件就可以。

1. **加锁顺序**：线程按照一定的顺序加锁和释放
2. **加锁时限**：线程尝试获取锁的时候加上一定的时限，超过时限则放弃对该锁的请求，并释放自己占有的锁



#### 如何排查死锁问题

工具： `pstack` + `gdb`

`pstack` 用于打印正在运行的进程的栈跟踪信息（在一段时间内，多次执行 `pstack` 如果代码栈总是同一个位置，则该段代码大概率出现问题【假死、死锁、死循环等】）

```bash
// gdb 命令
$ gdb -p 87746

// 打印所有的线程信息
(gdb) info thread
  3 Thread 0x7f60a610a700 (LWP 87747)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
  2 Thread 0x7f60a5709700 (LWP 87748)  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
* 1 Thread 0x7f60a610c700 (LWP 87746)  0x0000003720e080e5 in pthread_join () from /lib64/libpthread.so.0
//最左边的 * 表示 gdb 锁定的线程，切换到第二个线程去查看

// 切换到第2个线程
(gdb) thread 2
[Switching to thread 2 (Thread 0x7f60a5709700 (LWP 87748))]#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0 

// bt 可以打印函数堆栈，却无法看到函数参数，跟 pstack 命令一样 
// 也可以 pstack + 进程号
(gdb) bt
#0  0x0000003720e0da1d in __lll_lock_wait () from /lib64/libpthread.so.0
#1  0x0000003720e093ca in _L_lock_829 () from /lib64/libpthread.so.0
#2  0x0000003720e09298 in pthread_mutex_lock () from /lib64/libpthread.so.0
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
#4  0x0000003720e07893 in start_thread () from /lib64/libpthread.so.0
#5  0x00000037206f4bfd in clone () from /lib64/libc.so.6

// 打印第三帧信息，每次函数调用都会有压栈的过程，而 frame 则记录栈中的帧信息
(gdb) frame 3
#3  0x0000000000400792 in threadB_proc (data=0x0) at dead_lock.c:25
27    printf("thread B waiting get ResourceA \n");
28    pthread_mutex_lock(&mutex_A);

// 打印mutex_A的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_A
$1 = {__data = {__lock = 2, __count = 0, __owner = 87747, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\303V\001\000\001", '\000' <repeats 26 times>, __align = 2}

// 打印mutex_B的值 ,  __owner表示gdb中标示线程的值，即LWP
(gdb) p mutex_B
$2 = {__data = {__lock = 2, __count = 0, __owner = 87748, __nusers = 1, __kind = 0, __spins = 0, __list = {__prev = 0x0, __next = 0x0}}, 
  __size = "\002\000\000\000\000\000\000\000\304V\001\000\001", '\000' <repeats 26 times>, __align = 2}  
```





### 互斥锁与自旋锁

最底层的两种就是「互斥锁和自旋锁」，有很多高级的锁都是基于它们实现的

**如果能确定被锁住的代码执行时间很短，就不应该用互斥锁，而应该选用自旋锁，否则使用互斥锁。**

当已经有一个线程加锁后，其他线程加锁则就会失败，互斥锁和自旋锁对于加锁失败后的处理方式是不一样的：

- **互斥锁**加锁失败后，线程会**释放 CPU** ，给其他线程，也就是线程切换；
- **自旋锁**加锁失败后，线程会**忙等待**，直到它拿到锁；

**对于互斥锁加锁失败而阻塞的现象，是由操作系统内核实现的**。当加锁失败时，内核会将线程置为「睡眠」状态，等到锁被释放后，内核会在合适的时机唤醒线程，当这个线程成功获取到锁后，于是就可以继续执行。





# OS手撕相关

## 两个线程交替打印

用一个计数器 `cnt` 表示轮到哪个线程（奇数第一个线程，偶数第二个线程）打印，每个线程函数打印自己的东西，使用 `lock_guard(mutex)` 保护 `cnt`

**例题：**

1. 三个线程分别打印 A，B，C，要求这三个线程一起运行，打印 n 次，输出形如“ABCABCABC....”的字符串
2. 两个线程交替打印 0~100 的奇偶数
3. 通过 N 个线程顺序循环打印从 0 至 100
4. 多线程按顺序调用，A->B->C，AA 打印 5 次，BB 打印10 次，CC 打印 15 次，重复 10 次
5. 用两个线程，一个输出字母，一个输出数字，交替输出 1A2B3C4D...26Z

```cpp
// 两个线程交替打印AB
#include <iostream>
#include <mutex>
#include <thread>

using namespace std;

int cnt = 0;
mutex mtx;

void f1() {
    while (true) {
        lock_guard<mutex> lock(mtx);
        if (cnt % 2 == 0) {
            ++cnt;
            cout << "A";
        }
        if (cnt > 100)    break;
    }
}

void f2() {
    while (true) {
        lock_guard<mutex> lock(mtx);
        if (cnt % 2 == 1) {
            ++cnt;
            cout << "B";
        }
        if (cnt > 100)    break;
    }
}

int main() {
    thread t1(f1), t2(f2);
    t1.join();
    t2.join();

    return 0;
}
```



# Linux 常用命令



## 文件操作相关

1. "|"： 管道符“|”将两个命令隔开，管道符左边命令的输出就会作为管道符右边命令的输入。连续使用管道意味着第一个命令的输出会作为第二个命令的输入，第二个命令的输出又会作为第三个命令的输入，依此类推。
2. grep：-v 不显示匹配上的内容；-n 显示匹配上的内容
   - grep -v down，显示不包含down的内容。
   - grep -n down，显示包含down的内容。
3. du：（disk use）显示每个文件和目录的磁盘使用空间。
4. df：（disk free）显示磁盘分区上可以使用的磁盘空间。



### 查看文件内容的命令



```shell
vi  文件名 		#编辑方式查看，可修改
cat 文件名			#显示全部文件内容
more 文件名        #分页显示文件内容
less 文件名        #与more类似，可以向前翻页
tail 文件名        #仅查看尾部，还可以指定行数
head 文件名        #仅查看头部，还可以指定行数
```



### 搜索文件用什么命令？

```shell
find <指定目录> <指定条件> <指定动作>
whereis 加参数与文件名
locate 
```





### 移动文件用哪个命令？改名用哪个命令？



```shell
mv [选项] 源文件或目录 目标文件或目录
```

-b ：若需覆盖文件，则覆盖前先行备份。
-f ：force 强制的意思，如果目标文件已经存在，不会询问而直接覆盖；
-i ：若目标文件 (destination) 已经存在时，就会询问是否覆盖！
-u ：若目标文件已经存在，且 source 比较新，才会更新(update)
-t ： –target-directory=DIRECTORY move all SOURCE arguments into DIRECTORY，即指定mv的目标目录，该选项适用于移动多个源文件到一个目录的情况，此时目标目录在前，源文件在后。



### 创建目录



```shell
mkdir [-p] 目录名
```



### 删除文件



```
rm [-r]
rmdir [-r]
```



### linux 下命令有哪几种可使用的通配符？分别代表什么含义？

1. `?` ：可替代单个字符
2. `*` ：可替代任意多个字符
3. `[charset]` 可替代 charset 集中的任何单个字符，如 [a-z], [abABC]



### 用什么命令对一个文件的内容进行统计？



```shell
#wc 命令 -c 统计字节数 -l 统计行数 -w 统计字数
wc -c -l -w
```





## 文本匹配



### grep 命令有什么用？如何忽略大小写？如何查找不含该串的行？

```shell
grep [stringSTRING] filename
grep -v filename
```



## 性能查看相关





# 每日过一遍的知识点

- [ ] IO多路复用（select, poll, epoll）
- [ ] 进程/线程间的同步和通信方式
- [ ] 用户态和内核态是什么？是如何进行切换的？

